{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "mount_file_id": "1EM51knAqkgdgKR_LEsvL0B8NRH_-0_to",
      "authorship_tag": "ABX9TyO5yMW9TdBtRZ6e/hMKCKGn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bachaudhry/kaggle_birdCLEF_25/blob/main/BirdCLEF25_03_Baseline_Precomputed_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MPhonWFFUZEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.environ[\"KAGGLE_CONFIG_DIR\"] = \"/content/drive/MyDrive/Kaggle\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jOrk6FGUW24",
        "outputId": "978cfb04-f7b0-409c-f10f-1592a578d68a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle"
      ],
      "metadata": {
        "id": "QKPHMfMtgmSK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sys, gc, random, math, time, copy , zipfile, tarfile, shutil, subprocess, json\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "import IPython.display as ipd\n",
        "from IPython.display import display, clear_output\n",
        "import ipywidgets as widgets\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "import soundfile as sf\n",
        "\n",
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.multiprocessing as mp\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.amp as amp\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, average_precision_score\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "F0dtrnjeUFrv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append('/content/drive/MyDrive/Kaggle/Bird_CLEF25/utils')\n",
        "from utils import Config, BirdClefDataset, create_target_tensor, seed_everything, process_gzipped"
      ],
      "metadata": {
        "id": "4VY8jZA4fZWP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = Config()\n",
        "# Path to original train.csv, audio and metadata\n",
        "cfg.BASE_DATA_PATH = Path(\"/content/drive/MyDrive/Kaggle/Bird_CLEF25/data/birdclef-2025\")\n",
        "# Path to npy files\n",
        "cfg.PRECOMPUTED_SPECS_PATH = Path(\"/content/drive/MyDrive/Kaggle/Bird_CLEF25/data/precomputed-specs-np-zipped\")\n",
        "# Path to local specs\n",
        "#cfg.LOCAL_SPECS_PATH = Path(\"/content/precomputed_spectrograms\")\n",
        "# Training meta data\n",
        "cfg.TRAIN_METADATA_PATH = Path(\"/content/drive/MyDrive/Kaggle/Bird_CLEF25/data/birdclef-2025/train.csv\")"
      ],
      "metadata": {
        "id": "LmOAPiLPfZTo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Set Device & Seed ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "seed_everything(cfg.SEED)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYdzB_OafZQ_",
        "outputId": "7b272c37-30f3-4a64-a806-f98f7f0b68ea"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Seeded everything with: 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if cfg.NUM_WORKERS > 0:\n",
        "    try:\n",
        "        current_context = mp.get_context(None)\n",
        "        if not isinstance(current_context, mp.SpawnContext):\n",
        "             mp.set_start_method('spawn', force=True)\n",
        "             print(\"Set multiprocessing start method to 'spawn'.\")\n",
        "        else:\n",
        "             print(\"Multiprocessing start method already set to 'spawn'.\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Could not set start method (might be already set or first run): {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRsuayJcfZOS",
        "outputId": "341be319-7032-41eb-bff9-cf6fa5d2037b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Set multiprocessing start method to 'spawn'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load Metadata ---\n",
        "if not cfg.TRAIN_METADATA_PATH.exists():\n",
        "    print(f\"ERROR: Metadata file not found at {cfg.TRAIN_METADATA_PATH}\")\n",
        "    # Stop execution or handle\n",
        "else:\n",
        "    train_df = pd.read_csv(cfg.TRAIN_METADATA_PATH)\n",
        "    print(f\"Train metadata loaded. Shape: {train_df.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjDZIs1ZfZL7",
        "outputId": "216f5005-9bef-4f79-a337-694e4b5b2bc0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train metadata loaded. Shape: (28564, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import gdown\n",
        "## Testing improved download and unzip function\n",
        "#def process_gzippedV2(input_path, output_path=None):\n",
        "#  local_temp_dir = \"/content/temp_data\"\n",
        "  #local_extract_path = os.path.join(local_temp_dir, \"extracted\")\n",
        " # os.makedirs(local_extract_path, exist_ok=True)\n",
        "  #os.makedirs(local_temp_dir, exist_ok=True)\n",
        "\n",
        "  # Download using gdown - timeout issues\n",
        "  #print(\"Downloading compressed file from Google Drive...\")\n",
        "  #url = f'https://drive.google.com/uc?id={file_id.split(\"/\")[-2]}'\n",
        "  #compressed_path = gdown.download(url, output=local_temp_dir, quiet=False)\n",
        "  # Ensure compressed_path is a file, not the directory\n",
        "  #compressed_path = os.path.join(local_temp_dir, compressed_path)  # Corrected line\n",
        "\n",
        "    # Get filename and local paths\n",
        "  #filename = os.path.basename(input_path)\n",
        "  #local_compressed_path = os.path.join(local_temp_dir, filename)\n",
        "  #local_extract_path = os.path.join(local_temp_dir, 'extracted')\n",
        "  #os.makedirs(local_extract_path, exist_ok=True)\n",
        "\n",
        "  # Download file from Drive to Colab\n",
        "  #print(f\"Copying {filename} from Drive to Colab...\")\n",
        "  #drive_path = os.path.join('/content/drive/MyDrive/', input_path)\n",
        "  #shutil.copy2(drive_path, local_compressed_path) # Slower but more robust\n",
        "\n",
        "  # Extract with parallel decompression (if possible/available)\n",
        "  #print(f\"Extracting {os.path.basename(local_compressed_path)}...\")\n",
        "  #try:\n",
        "    # Using pigz for parallel decompression\n",
        "    #subprocess.run(['pigz', '--version'], check=True)\n",
        "    #subprocess.run(['tar', '-I', 'pigz', '-xf', local_compressed_path, '-C', local_extract_path],\n",
        "    #               check=True)\n",
        "  #except:\n",
        "    # Fallback to tar\n",
        "    #subprocess.run(['tar', '-xzf', local_compressed_path, '-C', local_extract_path],\n",
        "    #               check=True)\n",
        "\n",
        "  # Clean up compressed file\n",
        "  #os.remove(local_compressed_path)\n",
        "\n",
        "  # Optional upload to drive\n",
        "  #if output_path:\n",
        "       #print(\"‚è´ Starting Drive upload...\")\n",
        "        #drive_output_path = os.path.join('/content/drive/MyDrive', output_path)\n",
        "\n",
        "        # Use parallel upload with rsync\n",
        "        #subprocess.run([\n",
        "            #'rsync', '-a', '--info=progress2',\n",
        "            #local_extract_path + '/',\n",
        "            #drive_output_path\n",
        "        #], check=True)\n",
        "\n",
        "  #print(\"‚úÖ All operations completed!\")\n",
        "  #return local_extract_path\n"
      ],
      "metadata": {
        "id": "XBum5eud3TaK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#file_id = 'https://drive.google.com/file/d/1Ji5acgpHlyyhd8vI8gyQlN1nkjh16MwN/view?usp=drive_link'\n",
        "#input_path= \"/content/drive/MyDrive/Kaggle/Bird_CLEF25/data/precomputed-specs-np-zipped\"\n",
        "#ocal_extract = process_gzippedV2(input_path)"
      ],
      "metadata": {
        "id": "DoLJ2yFf3TUT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download zipped folder and extract to local\n",
        "input_path = \"/content/drive/MyDrive/Kaggle/Bird_CLEF25/data/precomputed-specs-np-zipped\"\n",
        "local_extract = process_gzipped(input_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "ynDHKvxkJMqN",
        "outputId": "7a23a08f-f121-4017-f54e-4be8a1650de9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying precomputed-specs-np-zipped from Drive to Colab...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-f200e69e03fe>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Download zipped folder and extract to local\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minput_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/Kaggle/Bird_CLEF25/data/precomputed-specs-np-zipped\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlocal_extract\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_gzipped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/drive/MyDrive/Kaggle/Bird_CLEF25/utils/utils.py\u001b[0m in \u001b[0;36mprocess_gzipped\u001b[0;34m(input_path, output_path)\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Copying {filename} from Drive to Colab...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0mdrive_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m   \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrive_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_compressed_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0;31m## Experiment with gdown -- Current run takes approx 14 mins\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36mcopy2\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m     \u001b[0mcopystat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    267\u001b[0m                     \u001b[0;32melif\u001b[0m \u001b[0m_USE_CP_SENDFILE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m                             \u001b[0m_fastcopy_sendfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfsrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m                             \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0m_GiveupOnFastCopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36m_fastcopy_sendfile\u001b[0;34m(fsrc, fdst)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;31m# ...in oder to have a more informative exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update precomputed specs path\n",
        "#cfg.PRECOMPUTED_SPECS_PATH = cfg.LOCAL_SPECS_PATH\n",
        "# Run check\n",
        "local_specs_path = Path(\"/content/temp_data/extracted/kaggle/working/precomputed_specs_np\")\n",
        "all_precomputed_files = list(local_specs_path.glob(\"*.npy\"))\n",
        "print(f\"Found {len(all_precomputed_files)} precomputed .npy files.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLlg8hWRfZJY",
        "outputId": "0d78feae-8148-44b4-817a-0a68ba7e1609"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 187904 precomputed .npy files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cfg.PRECOMPUTED_SPECS_PATH = local_specs_path"
      ],
      "metadata": {
        "id": "CQFu-UaLk3Lm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create label mappings\n",
        "unique_labels = sorted(train_df['primary_label'].unique())\n",
        "cfg.NUM_CLASSES = len(unique_labels)\n",
        "cfg.LABEL_TO_INT = {label: i for i, label in enumerate(unique_labels)}\n",
        "cfg.INT_TO_LABEL = {i: label for label, i in cfg.LABEL_TO_INT.items()}\n",
        "train_df['primary_label_int'] = train_df['primary_label'].map(cfg.LABEL_TO_INT)\n",
        "print(f\"{cfg.NUM_CLASSES} unique classes found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOOBGzhUfZEx",
        "outputId": "7ee61ea1-839f-45ad-9a8b-5651d3455927"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "206 unique classes found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "kCeUnrD6uTBB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This next step involves the creation of a manifest file to make the process of ensuring that preprocessed spectrograms are mapped to their respective originals in the train DF metadata file.\n",
        "\n",
        "If done correctly this cell will only need to be run once so that a `manifest.csv` file is created and stored in Google Drive. Afterwards, this step can be commented out and will be replaced with a new cell to simply load the manifest file from source."
      ],
      "metadata": {
        "id": "K6nQOKw083w7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "G1dHnCfpuWDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CREATE MANIFEST REVISED  ---\n",
        "\n",
        "# --- Load Metadata ---\n",
        "#if not cfg.TRAIN_METADATA_PATH.exists():\n",
        "#    print(f\"ERROR: Metadata file not found at {cfg.TRAIN_METADATA_PATH}\")\n",
        "#    raise FileNotFoundError(f\"Metadata missing: {cfg.TRAIN_METADATA_PATH}\")\n",
        "\n",
        "#train_df = pd.read_csv(cfg.TRAIN_METADATA_PATH)\n",
        "#print(f\"Train metadata loaded. Shape: {train_df.shape}\")\n",
        "\n",
        "# --- Create Label Mappings ---\n",
        "#unique_labels = sorted(train_df['primary_label'].unique())\n",
        "#cfg.NUM_CLASSES = len(unique_labels)\n",
        "#cfg.LABEL_TO_INT = {label: i for i, label in enumerate(unique_labels)}\n",
        "#cfg.INT_TO_LABEL = {i: label for label, i in cfg.LABEL_TO_INT.items()}\n",
        "#train_df['primary_label_int'] = train_df['primary_label'].map(cfg.LABEL_TO_INT)\n",
        "#print(f\"{cfg.NUM_CLASSES} unique classes found.\")\n",
        "\n",
        "# --- Scan Precomputed Files and Create clip_samples & Manifest ---\n",
        "#all_precomputed_files = list(cfg.PRECOMPUTED_SPECS_PATH.glob(\"*.npy\"))\n",
        "#print(f\"Found {len(all_precomputed_files)} precomputed .npy files.\")\n",
        "\n",
        "#clip_samples = []\n",
        "#manifest_data = []\n",
        "\n",
        "#if not all_precomputed_files:\n",
        "#    print(\"ERROR: No precomputed files found.\")\n",
        "#else:\n",
        "#    print(\"Generating clip_info_list and manifest from precomputed files and train_df...\")\n",
        "\n",
        "    # --- Create the lookup dictionary ---\n",
        "    # Key: The part of the .npy filename BEFORE \"_clipIDX\"\n",
        "    # Value: (primary_label_int, original_filename_from_train_df)\n",
        "#    filename_to_label_map = {}\n",
        "#    for _, row in train_df.iterrows():\n",
        "#        original_filename_from_train_df = row['filename']\n",
        "        # THIS IS THE KEY ASSUMPTION:\n",
        "        # The key should be the original filename as it appears in train_df,\n",
        "        # if your .npy files are like \"TRAIN_DF_FILENAME_clipIDX.npy\"\n",
        "        # If train_df filenames have '/', and .npy files have them replaced with '_', adjust here.\n",
        "        # For now, assume .npy naming directly uses train_df['filename'] (potentially with slashes replaced)\n",
        "\n",
        "        # Let's assume your .npy files look like: \"original_filename_from_train_df_WITH_SLASHES_REPLACED_clipIDX.npy\"\n",
        "        # AND that \".ogg\" is part of this stem in the .npy file.\n",
        "#        key_for_map = original_filename_from_train_df.replace('/', '_') # Example: \"subdir_file.ogg\"\n",
        "\n",
        "#        filename_to_label_map[key_for_map] = (row['primary_label_int'], original_filename_from_train_df)\n",
        "\n",
        "    # --- DEBUG: Print some keys from the map ---\n",
        " #   print(f\"Generated {len(filename_to_label_map)} keys for lookup map.\")\n",
        " #   print(\"Sample keys from filename_to_label_map (first 5):\")\n",
        " #   for i, key in enumerate(filename_to_label_map.keys()):\n",
        " #       if i < 5:\n",
        " #           print(f\"  '{key}' -> {filename_to_label_map[key]}\")\n",
        " #       else:\n",
        " #           break\n",
        "    # --- END DEBUG ---\n",
        "\n",
        " #   processed_count = 0\n",
        " #   for spec_path in tqdm(all_precomputed_files, desc=\"Mapping precomputed files\"):\n",
        " #       try:\n",
        " #           npy_filename_stem_full = spec_path.stem # e.g., \"21211_XC934741.ogg_clip0\"\n",
        " #           parts = npy_filename_stem_full.rsplit('_clip', 1) # Use rsplit to split on the *last* occurrence\n",
        "\n",
        " #           if len(parts) != 2:\n",
        " #               print(f\"Warning: Could not parse clip index from '{spec_path.name}' using '_clip'. Skipping. Full stem: '{npy_filename_stem_full}'\")\n",
        " #               continue\n",
        "\n",
        " #           parsed_name_stem_from_npy = parts[0] # This should be the key we look up, e.g., \"21211_XC934741.ogg\"\n",
        "                                                # or \"ebird_code_XC12345.ogg\"\n",
        "\n",
        "            # --- DEBUG: Print parsed stem from .npy ---\n",
        " #           if processed_count < 5: # Print for the first 5 .npy files\n",
        " #                print(f\"  Attempting to match .npy parsed stem: '{parsed_name_stem_from_npy}'\")\n",
        "            # --- END DEBUG ---\n",
        "\n",
        " #           clip_idx_str = parts[1]\n",
        " #           if not clip_idx_str.isdigit():\n",
        " #               print(f\"Warning: Clip index part '{clip_idx_str}' is not a digit in '{spec_path.name}'. Skipping.\")\n",
        " #               continue\n",
        " #           clip_idx = int(clip_idx_str)\n",
        "\n",
        "\n",
        "#            if parsed_name_stem_from_npy in filename_to_label_map:\n",
        "#                primary_label_int, original_full_filename = filename_to_label_map[parsed_name_stem_from_npy]\n",
        "\n",
        "#                clip_entry = {\n",
        "#                    'original_filename': original_full_filename,\n",
        "#                    'spec_npy_filename': spec_path.name,\n",
        "#                    'clip_index': clip_idx,\n",
        "#                    'primary_label_int': primary_label_int\n",
        "#                }\n",
        "#                clip_samples.append(clip_entry)\n",
        "#                manifest_data.append(clip_entry)\n",
        "#            else:\n",
        "#                if processed_count < 20: # Print warning only for the first few mismatches to avoid flooding\n",
        "#                    print(f\"Warning: No match in train_df lookup for parsed .npy stem '{parsed_name_stem_from_npy}' from file '{spec_path.name}'\")\n",
        "\n",
        "#            processed_count += 1\n",
        "\n",
        "#        except Exception as e:\n",
        "#            print(f\"Error parsing or mapping {spec_path.name}: {e}\")\n",
        "\n",
        "#    cfg.TOTAL_CLIPS = len(clip_samples)\n",
        "#    print(f\"Created {cfg.TOTAL_CLIPS} clip samples from precomputed files.\")\n",
        "\n",
        "#    if not clip_samples:\n",
        "#         print(\"CRITICAL ERROR: clip_samples list is empty. Check mapping logic and .npy filenames based on debug output.\")\n",
        "#    else:\n",
        "#        manifest_df = pd.DataFrame(manifest_data)\n",
        "#        manifest_save_path = \"/content/temp_data/manifest.csv\"\n",
        "#        try:\n",
        "#            manifest_df.to_csv(manifest_save_path, index=False)\n",
        "#            print(f\"Manifest file saved to: {manifest_save_path}\")\n",
        "#            print(manifest_df.head())\n",
        "#        except Exception as e:\n",
        "#            print(f\"Error saving manifest to {manifest_save_path}: {e}\")\n",
        "#            print(\"Attempting to save to /content/manifest.csv instead...\")\n",
        "#            manifest_save_path_content = Path(\"/content/manifest.csv\")\n",
        "#            manifest_df.to_csv(manifest_save_path_content, index=False)\n",
        "#            print(f\"Manifest file saved to: {manifest_save_path_content}\")\n",
        "#            print(manifest_df.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "bEbDN7Y8EZRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load manifest data\n",
        "manifest_load_path = Path(\"/content/drive/MyDrive/Kaggle/Bird_CLEF25/data/manifest.csv\")\n",
        "if manifest_load_path.exists():\n",
        "    manifest_df = pd.read_csv(manifest_load_path)\n",
        "    print(f\"Manifest loaded from {manifest_load_path}\")\n",
        "    print(manifest_df.head(15))\n",
        "\n",
        "    # Ensure correct data types\n",
        "    manifest_df['clip_index'] = manifest_df['clip_index'].astype(int)\n",
        "    manifest_df['primary_label_int'] = manifest_df['primary_label_int'].astype(int)\n",
        "\n",
        "    clip_samples = manifest_df.to_dict('records') # Convert DF rows to list of dicts\n",
        "    cfg.TOTAL_CLIPS = len(clip_samples)\n",
        "    print(f\"Loaded {cfg.TOTAL_CLIPS} clip samples from mainfest.\")\n",
        "    print(\"Sample loaded clip_info: \", clip_samples[0] if clip_samples else \"N/A\")\n",
        "else:\n",
        "  print(f\"Error: Manifest not found at {manifest_load_path}\")\n",
        "  raise FileNotFoundError(f\"Manifest missing: {manifest_load_path}\")"
      ],
      "metadata": {
        "id": "sovL4wAy_I-o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c1ca14f-6632-48b3-89ee-720031e20dec"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manifest loaded from /content/drive/MyDrive/Kaggle/Bird_CLEF25/data/manifest.csv\n",
            "          original_filename                  spec_npy_filename  clip_index  \\\n",
            "0       strowl1/XC48736.ogg      strowl1_XC48736.ogg_clip1.npy           1   \n",
            "1       roahaw/XC113755.ogg      roahaw_XC113755.ogg_clip1.npy           1   \n",
            "2       bkcdon/XC250791.ogg      bkcdon_XC250791.ogg_clip2.npy           2   \n",
            "3      bkmtou1/XC821017.ogg    bkmtou1_XC821017.ogg_clip17.npy          17   \n",
            "4      yebsee1/XC216248.ogg    yebsee1_XC216248.ogg_clip11.npy          11   \n",
            "5       roahaw/XC925789.ogg     roahaw_XC925789.ogg_clip16.npy          16   \n",
            "6   grbhaw1/iNat1156713.ogg  grbhaw1_iNat1156713.ogg_clip0.npy           0   \n",
            "7       grnkin/XC452703.ogg      grnkin_XC452703.ogg_clip1.npy           1   \n",
            "8      paltan1/XC375634.ogg    paltan1_XC375634.ogg_clip27.npy          27   \n",
            "9      cotfly1/XC817211.ogg     cotfly1_XC817211.ogg_clip2.npy           2   \n",
            "10     wbwwre1/XC456410.ogg     wbwwre1_XC456410.ogg_clip0.npy           0   \n",
            "11      whtdov/XC551074.ogg     whtdov_XC551074.ogg_clip10.npy          10   \n",
            "12   littin1/iNat153503.ogg   littin1_iNat153503.ogg_clip5.npy           5   \n",
            "13     linwoo1/XC344481.ogg     linwoo1_XC344481.ogg_clip9.npy           9   \n",
            "14     srwswa1/XC518146.ogg    srwswa1_XC518146.ogg_clip11.npy          11   \n",
            "\n",
            "    primary_label_int  \n",
            "0                 175  \n",
            "1                 144  \n",
            "2                  70  \n",
            "3                  71  \n",
            "4                 197  \n",
            "5                 144  \n",
            "6                 109  \n",
            "7                 116  \n",
            "8                 131  \n",
            "9                  99  \n",
            "10                185  \n",
            "11                190  \n",
            "12                124  \n",
            "13                123  \n",
            "14                170  \n",
            "Loaded 187904 clip samples from mainfest.\n",
            "Sample loaded clip_info:  {'original_filename': 'strowl1/XC48736.ogg', 'spec_npy_filename': 'strowl1_XC48736.ogg_clip1.npy', 'clip_index': 1, 'primary_label_int': 175}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training / Validation Split"
      ],
      "metadata": {
        "id": "Huti6zxLD9pY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'clip_samples' not in globals() or not clip_samples:\n",
        "  print(\"Error: clip_samples is not loaded, run the previous steps first!\")\n",
        "  raise ValueError(\"clip_samples not loaded. Can't proceed with data splitting.\")\n",
        "else:\n",
        "  clip_df_for_split = pd.DataFrame(clip_samples)\n",
        "\n",
        "  # Filter classes with only one sample - for stratification\n",
        "  label_counts = clip_df_for_split['primary_label_int'].value_counts()\n",
        "  single_sample_labels = label_counts[label_counts == 1].index.tolist()\n",
        "\n",
        "  if single_sample_labels:\n",
        "    print(f\"Found {len(single_sample_labels)} classes with only 1 precomputed sample clip.\")\n",
        "    # Classes being removed\n",
        "    clip_df_filtered = clip_df_for_split[~clip_df_for_split['primary_label_int'].isin(single_sample_labels)].copy()\n",
        "    removed_count = len(clip_df_for_split) - len(clip_df_filtered)\n",
        "    print(f\"Removed {removed_count} clips belonging to single sample classes\")\n",
        "    print(f\"Remaining clips for splitting: {len(clip_df_filtered)}\")\n",
        "  else:\n",
        "    clip_df_filtered = clip_df_for_split.copy()\n",
        "    print(\"No classes with only 1 precomputed sample clip found. No filtering applied.\")\n",
        "\n",
        "  if not clip_df_filtered.empty:\n",
        "    # Indeces of the filteredd dataframe for splitting\n",
        "    features = clip_df_filtered.index\n",
        "    labels = clip_df_filtered['primary_label_int']\n",
        "\n",
        "    try:\n",
        "      train_indeces, val_indeces = train_test_split(\n",
        "          features, # split on the df index\n",
        "          test_size=0.2,\n",
        "          random_state=cfg.SEED,\n",
        "          stratify=labels\n",
        "      )\n",
        "      # Train and validation lists of clip info dicts\n",
        "      train_clip_info = clip_df_filtered.loc[train_indeces].to_dict('records')\n",
        "      val_clip_info = clip_df_filtered.loc[val_indeces].to_dict('records')\n",
        "\n",
        "      print(f\"Training clips: {len(train_clip_info)}\")\n",
        "      print(f\"Validation clips: {len(val_clip_info)}\")\n",
        "\n",
        "      # Verify stratification - check distributions in train and val\n",
        "      train_labels_dist = pd.Series([d['primary_label_int'] for d in train_clip_info]).value_counts(normalize=True)\n",
        "      val_labels_dist = pd.Series([d['primary_label_int'] for d in val_clip_info]).value_counts(normalize=True)\n",
        "      print(\"Example class proportions: \")\n",
        "\n",
        "      if cfg.LABEL_TO_INT:\n",
        "          example_class_name = list(cfg.LABEL_TO_INT.keys())[0] # Get first class name\n",
        "          example_class_int = cfg.LABEL_TO_INT.get(example_class_name)\n",
        "          if example_class_int is not None and example_class_int not in single_sample_labels:\n",
        "              print(f\"  Class '{example_class_name}' (Index {example_class_int}):\")\n",
        "              print(f\"    Train proportion: {train_labels_dist.get(example_class_int, 0):.4f}\")\n",
        "              print(f\"    Valid proportion: {val_labels_dist.get(example_class_int, 0):.4f}\")\n",
        "          else:\n",
        "              print(f\"  Cannot verify example class '{example_class_name}' (not found or was removed).\")\n",
        "      else:\n",
        "        print(\"  Label mappings (cfg.LABEL_TO_INT) not available for verification.\")\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(f\"\\nError during stratified split: {e}\")\n",
        "        print(\"This can happen if, even after filtering, some classes have too few members for the split ratio.\")\n",
        "        print(\"Consider a smaller validation split or further data cleaning if this persists.\")\n",
        "        train_clip_info, val_clip_info = [], [] # Ensure they are empty on error\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Do-1YF2F-F8L",
        "outputId": "88ba194b-4d80-4aef-be8c-68bee59b4304"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 classes with only 1 precomputed sample clip.\n",
            "Removed 1 clips belonging to single sample classes\n",
            "Remaining clips for splitting: 187903\n",
            "Training clips: 150322\n",
            "Validation clips: 37581\n",
            "Example class proportions: \n",
            "  Class '1139490' (Index 0):\n",
            "    Train proportion: 0.0002\n",
            "    Valid proportion: 0.0002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loaders"
      ],
      "metadata": {
        "id": "7ZZmUfrlJbpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg.BATCH_SIZE = 128 # test 256 if memory allows\n",
        "cfg.NUM_WORKERS = 2 # test 8 if config allows"
      ],
      "metadata": {
        "id": "-d8n0Wbr-F5r"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure time steps is correctly calculated from the precomputation parameters\n",
        "calculated_time_steps = math.floor(cfg.TARGET_SAMPLES / cfg.HOP_LENGTH) + 1\n",
        "calculated_time_steps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsdkTf4D-F3O",
        "outputId": "00897729-5d4d-496e-96ad-6c3ab56a0ecb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "313"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = BirdClefDataset(\n",
        "    train_clip_info,\n",
        "    cfg.PRECOMPUTED_SPECS_PATH,\n",
        "    cfg.NUM_CLASSES,\n",
        "    cfg.N_MELS,\n",
        "    calculated_time_steps,\n",
        "    augmentations=None\n",
        ")\n",
        "\n",
        "val_dataset = BirdClefDataset(\n",
        "    val_clip_info,\n",
        "    cfg.PRECOMPUTED_SPECS_PATH,\n",
        "    cfg.NUM_CLASSES,\n",
        "    cfg.N_MELS,\n",
        "    calculated_time_steps,\n",
        "    augmentations=None\n",
        ")"
      ],
      "metadata": {
        "id": "fgF3gkxg-F0m"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Train Dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation Dataset size: {len(val_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QtVNAOE-FyO",
        "outputId": "8fd91c0c-97f6-4769-e52b-5a96db91a919"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset size: 150322\n",
            "Validation Dataset size: 37581\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    cfg.BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=cfg.NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    cfg.BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=cfg.NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    drop_last=False\n",
        ")\n",
        "\n",
        "print(f\"DataLoaders created. Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H20KbmKV-FdZ",
        "outputId": "0d0596b3-f704-4ac6-9d2a-2246d8df8b3b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataLoaders created. Train batches: 1174, Val batches: 294\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    spec_batch, label_batch = next(iter(train_loader))\n",
        "    print(f\"  Spectrogram batch shape: {spec_batch.shape}\")\n",
        "    print(f\"  Label batch shape: {label_batch.shape}\")\n",
        "    print(f\"  Spectrogram device: {spec_batch.device}\") # Expect 'cpu'\n",
        "    print(f\"  Spectrogram dtype: {spec_batch.dtype}\")\n",
        "    print(f\"  Label device: {label_batch.device}\")     # Expect 'cpu'\n",
        "    print(f\"  Label dtype: {label_batch.dtype}\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR fetching batch from DataLoader: {e}\")\n",
        "    print(\"Common issues: issues in Dataset __getitem__, multiprocessing setup, or data corruption.\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKt1DnKS-Fa1",
        "outputId": "f0cf0129-06fb-4fea-c750-5b71db5f5d59"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Spectrogram batch shape: torch.Size([128, 128, 313])\n",
            "  Label batch shape: torch.Size([128, 206])\n",
            "  Spectrogram device: cpu\n",
            "  Spectrogram dtype: torch.float32\n",
            "  Label device: cpu\n",
            "  Label dtype: torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model, Loss, Optimizer, Metrics"
      ],
      "metadata": {
        "id": "r6hdJOW7Pl2O"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MbUK1Hg5-FYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6BWMK5Ri-FWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8BE5Vlal-FTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q9kPX1oi-FRN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}