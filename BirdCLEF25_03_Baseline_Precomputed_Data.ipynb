{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "mount_file_id": "1EM51knAqkgdgKR_LEsvL0B8NRH_-0_to",
      "authorship_tag": "ABX9TyNsYf6yBaJF13Vx7I9gwslp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bachaudhry/kaggle_birdCLEF_25/blob/main/BirdCLEF25_03_Baseline_Precomputed_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MPhonWFFUZEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.environ[\"KAGGLE_CONFIG_DIR\"] = \"/content/drive/MyDrive/Kaggle\""
      ],
      "metadata": {
        "id": "6jOrk6FGUW24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle"
      ],
      "metadata": {
        "id": "QKPHMfMtgmSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sys, gc, random, math, time, copy , zipfile, tarfile, shutil, subprocess, json\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "import IPython.display as ipd\n",
        "from IPython.display import display, clear_output\n",
        "import ipywidgets as widgets\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "import soundfile as sf\n",
        "\n",
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.multiprocessing as mp\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.amp as amp\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, average_precision_score\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "F0dtrnjeUFrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append('/content/drive/MyDrive/Kaggle/Bird_CLEF25/utils')\n",
        "from utils import Config, BirdClefDataset, create_target_tensor, seed_everything, process_gzipped"
      ],
      "metadata": {
        "id": "4VY8jZA4fZWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = Config()\n",
        "# Path to original train.csv, audio and metadata\n",
        "cfg.BASE_DATA_PATH = Path(\"/content/drive/MyDrive/Kaggle/Bird_CLEF25/data/birdclef-2025\")\n",
        "# Path to npy files\n",
        "cfg.PRECOMPUTED_SPECS_PATH = Path(\"/content/drive/MyDrive/Kaggle/Bird_CLEF25/data/precomputed-specs-np-zipped\")\n",
        "# Path to local specs\n",
        "#cfg.LOCAL_SPECS_PATH = Path(\"/content/precomputed_spectrograms\")\n",
        "# Training meta data\n",
        "cfg.TRAIN_METADATA_PATH = Path(\"/content/drive/MyDrive/Kaggle/Bird_CLEF25/data/birdclef-2025/train.csv\")"
      ],
      "metadata": {
        "id": "LmOAPiLPfZTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Set Device & Seed ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "seed_everything(cfg.SEED)"
      ],
      "metadata": {
        "id": "QYdzB_OafZQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if cfg.NUM_WORKERS > 0:\n",
        "    try:\n",
        "        current_context = mp.get_context(None)\n",
        "        if not isinstance(current_context, mp.SpawnContext):\n",
        "             mp.set_start_method('spawn', force=True)\n",
        "             print(\"Set multiprocessing start method to 'spawn'.\")\n",
        "        else:\n",
        "             print(\"Multiprocessing start method already set to 'spawn'.\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Could not set start method (might be already set or first run): {e}\")"
      ],
      "metadata": {
        "id": "ZRsuayJcfZOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load Metadata ---\n",
        "if not cfg.TRAIN_METADATA_PATH.exists():\n",
        "    print(f\"ERROR: Metadata file not found at {cfg.TRAIN_METADATA_PATH}\")\n",
        "    # Stop execution or handle\n",
        "else:\n",
        "    train_df = pd.read_csv(cfg.TRAIN_METADATA_PATH)\n",
        "    print(f\"Train metadata loaded. Shape: {train_df.shape}\")"
      ],
      "metadata": {
        "id": "RjDZIs1ZfZL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import gdown\n",
        "## Testing improved download and unzip function\n",
        "#def process_gzippedV2(input_path, output_path=None):\n",
        "#  local_temp_dir = \"/content/temp_data\"\n",
        "  #local_extract_path = os.path.join(local_temp_dir, \"extracted\")\n",
        " # os.makedirs(local_extract_path, exist_ok=True)\n",
        "  #os.makedirs(local_temp_dir, exist_ok=True)\n",
        "\n",
        "  # Download using gdown - timeout issues\n",
        "  #print(\"Downloading compressed file from Google Drive...\")\n",
        "  #url = f'https://drive.google.com/uc?id={file_id.split(\"/\")[-2]}'\n",
        "  #compressed_path = gdown.download(url, output=local_temp_dir, quiet=False)\n",
        "  # Ensure compressed_path is a file, not the directory\n",
        "  #compressed_path = os.path.join(local_temp_dir, compressed_path)  # Corrected line\n",
        "\n",
        "    # Get filename and local paths\n",
        "  #filename = os.path.basename(input_path)\n",
        "  #local_compressed_path = os.path.join(local_temp_dir, filename)\n",
        "  #local_extract_path = os.path.join(local_temp_dir, 'extracted')\n",
        "  #os.makedirs(local_extract_path, exist_ok=True)\n",
        "\n",
        "  # Download file from Drive to Colab\n",
        "  #print(f\"Copying {filename} from Drive to Colab...\")\n",
        "  #drive_path = os.path.join('/content/drive/MyDrive/', input_path)\n",
        "  #shutil.copy2(drive_path, local_compressed_path) # Slower but more robust\n",
        "\n",
        "  # Extract with parallel decompression (if possible/available)\n",
        "  #print(f\"Extracting {os.path.basename(local_compressed_path)}...\")\n",
        "  #try:\n",
        "    # Using pigz for parallel decompression\n",
        "    #subprocess.run(['pigz', '--version'], check=True)\n",
        "    #subprocess.run(['tar', '-I', 'pigz', '-xf', local_compressed_path, '-C', local_extract_path],\n",
        "    #               check=True)\n",
        "  #except:\n",
        "    # Fallback to tar\n",
        "    #subprocess.run(['tar', '-xzf', local_compressed_path, '-C', local_extract_path],\n",
        "    #               check=True)\n",
        "\n",
        "  # Clean up compressed file\n",
        "  #os.remove(local_compressed_path)\n",
        "\n",
        "  # Optional upload to drive\n",
        "  #if output_path:\n",
        "       #print(\"⏫ Starting Drive upload...\")\n",
        "        #drive_output_path = os.path.join('/content/drive/MyDrive', output_path)\n",
        "\n",
        "        # Use parallel upload with rsync\n",
        "        #subprocess.run([\n",
        "            #'rsync', '-a', '--info=progress2',\n",
        "            #local_extract_path + '/',\n",
        "            #drive_output_path\n",
        "        #], check=True)\n",
        "\n",
        "  #print(\"✅ All operations completed!\")\n",
        "  #return local_extract_path\n"
      ],
      "metadata": {
        "id": "XBum5eud3TaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#file_id = 'https://drive.google.com/file/d/1Ji5acgpHlyyhd8vI8gyQlN1nkjh16MwN/view?usp=drive_link'\n",
        "#input_path= \"/content/drive/MyDrive/Kaggle/Bird_CLEF25/data/precomputed-specs-np-zipped\"\n",
        "#ocal_extract = process_gzippedV2(input_path)"
      ],
      "metadata": {
        "id": "DoLJ2yFf3TUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download zipped folder and extract to local # One time per setup\n",
        "#input_path = \"/content/drive/MyDrive/Kaggle/Bird_CLEF25/data/precomputed-specs-np-zipped\"\n",
        "#local_extract = process_gzipped(input_path)"
      ],
      "metadata": {
        "id": "ynDHKvxkJMqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update precomputed specs path\n",
        "#cfg.PRECOMPUTED_SPECS_PATH = cfg.LOCAL_SPECS_PATH\n",
        "# Run check\n",
        "local_specs_path = Path(\"/content/temp_data/extracted/kaggle/working/precomputed_specs_np\")\n",
        "all_precomputed_files = list(local_specs_path.glob(\"*.npy\"))\n",
        "print(f\"Found {len(all_precomputed_files)} precomputed .npy files.\")"
      ],
      "metadata": {
        "id": "kLlg8hWRfZJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg.PRECOMPUTED_SPECS_PATH = local_specs_path"
      ],
      "metadata": {
        "id": "CQFu-UaLk3Lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create label mappings\n",
        "unique_labels = sorted(train_df['primary_label'].unique())\n",
        "cfg.NUM_CLASSES = len(unique_labels)\n",
        "cfg.LABEL_TO_INT = {label: i for i, label in enumerate(unique_labels)}\n",
        "cfg.INT_TO_LABEL = {i: label for label, i in cfg.LABEL_TO_INT.items()}\n",
        "train_df['primary_label_int'] = train_df['primary_label'].map(cfg.LABEL_TO_INT)\n",
        "print(f\"{cfg.NUM_CLASSES} unique classes found.\")"
      ],
      "metadata": {
        "id": "YOOBGzhUfZEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "kCeUnrD6uTBB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This next step involves the creation of a manifest file to make the process of ensuring that preprocessed spectrograms are mapped to their respective originals in the train DF metadata file.\n",
        "\n",
        "If done correctly this cell will only need to be run once so that a `manifest.csv` file is created and stored in Google Drive. Afterwards, this step can be commented out and will be replaced with a new cell to simply load the manifest file from source."
      ],
      "metadata": {
        "id": "K6nQOKw083w7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "G1dHnCfpuWDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CREATE MANIFEST REVISED  ---\n",
        "\n",
        "# --- Load Metadata ---\n",
        "#if not cfg.TRAIN_METADATA_PATH.exists():\n",
        "#    print(f\"ERROR: Metadata file not found at {cfg.TRAIN_METADATA_PATH}\")\n",
        "#    raise FileNotFoundError(f\"Metadata missing: {cfg.TRAIN_METADATA_PATH}\")\n",
        "\n",
        "#train_df = pd.read_csv(cfg.TRAIN_METADATA_PATH)\n",
        "#print(f\"Train metadata loaded. Shape: {train_df.shape}\")\n",
        "\n",
        "# --- Create Label Mappings ---\n",
        "#unique_labels = sorted(train_df['primary_label'].unique())\n",
        "#cfg.NUM_CLASSES = len(unique_labels)\n",
        "#cfg.LABEL_TO_INT = {label: i for i, label in enumerate(unique_labels)}\n",
        "#cfg.INT_TO_LABEL = {i: label for label, i in cfg.LABEL_TO_INT.items()}\n",
        "#train_df['primary_label_int'] = train_df['primary_label'].map(cfg.LABEL_TO_INT)\n",
        "#print(f\"{cfg.NUM_CLASSES} unique classes found.\")\n",
        "\n",
        "# --- Scan Precomputed Files and Create clip_samples & Manifest ---\n",
        "#all_precomputed_files = list(cfg.PRECOMPUTED_SPECS_PATH.glob(\"*.npy\"))\n",
        "#print(f\"Found {len(all_precomputed_files)} precomputed .npy files.\")\n",
        "\n",
        "#clip_samples = []\n",
        "#manifest_data = []\n",
        "\n",
        "#if not all_precomputed_files:\n",
        "#    print(\"ERROR: No precomputed files found.\")\n",
        "#else:\n",
        "#    print(\"Generating clip_info_list and manifest from precomputed files and train_df...\")\n",
        "\n",
        "    # --- Create the lookup dictionary ---\n",
        "    # Key: The part of the .npy filename BEFORE \"_clipIDX\"\n",
        "    # Value: (primary_label_int, original_filename_from_train_df)\n",
        "#    filename_to_label_map = {}\n",
        "#    for _, row in train_df.iterrows():\n",
        "#        original_filename_from_train_df = row['filename']\n",
        "        # THIS IS THE KEY ASSUMPTION:\n",
        "        # The key should be the original filename as it appears in train_df,\n",
        "        # if your .npy files are like \"TRAIN_DF_FILENAME_clipIDX.npy\"\n",
        "        # If train_df filenames have '/', and .npy files have them replaced with '_', adjust here.\n",
        "        # For now, assume .npy naming directly uses train_df['filename'] (potentially with slashes replaced)\n",
        "\n",
        "        # Let's assume your .npy files look like: \"original_filename_from_train_df_WITH_SLASHES_REPLACED_clipIDX.npy\"\n",
        "        # AND that \".ogg\" is part of this stem in the .npy file.\n",
        "#        key_for_map = original_filename_from_train_df.replace('/', '_') # Example: \"subdir_file.ogg\"\n",
        "\n",
        "#        filename_to_label_map[key_for_map] = (row['primary_label_int'], original_filename_from_train_df)\n",
        "\n",
        "    # --- DEBUG: Print some keys from the map ---\n",
        " #   print(f\"Generated {len(filename_to_label_map)} keys for lookup map.\")\n",
        " #   print(\"Sample keys from filename_to_label_map (first 5):\")\n",
        " #   for i, key in enumerate(filename_to_label_map.keys()):\n",
        " #       if i < 5:\n",
        " #           print(f\"  '{key}' -> {filename_to_label_map[key]}\")\n",
        " #       else:\n",
        " #           break\n",
        "    # --- END DEBUG ---\n",
        "\n",
        " #   processed_count = 0\n",
        " #   for spec_path in tqdm(all_precomputed_files, desc=\"Mapping precomputed files\"):\n",
        " #       try:\n",
        " #           npy_filename_stem_full = spec_path.stem # e.g., \"21211_XC934741.ogg_clip0\"\n",
        " #           parts = npy_filename_stem_full.rsplit('_clip', 1) # Use rsplit to split on the *last* occurrence\n",
        "\n",
        " #           if len(parts) != 2:\n",
        " #               print(f\"Warning: Could not parse clip index from '{spec_path.name}' using '_clip'. Skipping. Full stem: '{npy_filename_stem_full}'\")\n",
        " #               continue\n",
        "\n",
        " #           parsed_name_stem_from_npy = parts[0] # This should be the key we look up, e.g., \"21211_XC934741.ogg\"\n",
        "                                                # or \"ebird_code_XC12345.ogg\"\n",
        "\n",
        "            # --- DEBUG: Print parsed stem from .npy ---\n",
        " #           if processed_count < 5: # Print for the first 5 .npy files\n",
        " #                print(f\"  Attempting to match .npy parsed stem: '{parsed_name_stem_from_npy}'\")\n",
        "            # --- END DEBUG ---\n",
        "\n",
        " #           clip_idx_str = parts[1]\n",
        " #           if not clip_idx_str.isdigit():\n",
        " #               print(f\"Warning: Clip index part '{clip_idx_str}' is not a digit in '{spec_path.name}'. Skipping.\")\n",
        " #               continue\n",
        " #           clip_idx = int(clip_idx_str)\n",
        "\n",
        "\n",
        "#            if parsed_name_stem_from_npy in filename_to_label_map:\n",
        "#                primary_label_int, original_full_filename = filename_to_label_map[parsed_name_stem_from_npy]\n",
        "\n",
        "#                clip_entry = {\n",
        "#                    'original_filename': original_full_filename,\n",
        "#                    'spec_npy_filename': spec_path.name,\n",
        "#                    'clip_index': clip_idx,\n",
        "#                    'primary_label_int': primary_label_int\n",
        "#                }\n",
        "#                clip_samples.append(clip_entry)\n",
        "#                manifest_data.append(clip_entry)\n",
        "#            else:\n",
        "#                if processed_count < 20: # Print warning only for the first few mismatches to avoid flooding\n",
        "#                    print(f\"Warning: No match in train_df lookup for parsed .npy stem '{parsed_name_stem_from_npy}' from file '{spec_path.name}'\")\n",
        "\n",
        "#            processed_count += 1\n",
        "\n",
        "#        except Exception as e:\n",
        "#            print(f\"Error parsing or mapping {spec_path.name}: {e}\")\n",
        "\n",
        "#    cfg.TOTAL_CLIPS = len(clip_samples)\n",
        "#    print(f\"Created {cfg.TOTAL_CLIPS} clip samples from precomputed files.\")\n",
        "\n",
        "#    if not clip_samples:\n",
        "#         print(\"CRITICAL ERROR: clip_samples list is empty. Check mapping logic and .npy filenames based on debug output.\")\n",
        "#    else:\n",
        "#        manifest_df = pd.DataFrame(manifest_data)\n",
        "#        manifest_save_path = \"/content/temp_data/manifest.csv\"\n",
        "#        try:\n",
        "#            manifest_df.to_csv(manifest_save_path, index=False)\n",
        "#            print(f\"Manifest file saved to: {manifest_save_path}\")\n",
        "#            print(manifest_df.head())\n",
        "#        except Exception as e:\n",
        "#            print(f\"Error saving manifest to {manifest_save_path}: {e}\")\n",
        "#            print(\"Attempting to save to /content/manifest.csv instead...\")\n",
        "#            manifest_save_path_content = Path(\"/content/manifest.csv\")\n",
        "#            manifest_df.to_csv(manifest_save_path_content, index=False)\n",
        "#            print(f\"Manifest file saved to: {manifest_save_path_content}\")\n",
        "#            print(manifest_df.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "bEbDN7Y8EZRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load manifest data\n",
        "manifest_load_path = Path(\"/content/drive/MyDrive/Kaggle/Bird_CLEF25/data/manifest.csv\")\n",
        "if manifest_load_path.exists():\n",
        "    manifest_df = pd.read_csv(manifest_load_path)\n",
        "    print(f\"Manifest loaded from {manifest_load_path}\")\n",
        "    print(manifest_df.head(15))\n",
        "\n",
        "    # Ensure correct data types\n",
        "    manifest_df['clip_index'] = manifest_df['clip_index'].astype(int)\n",
        "    manifest_df['primary_label_int'] = manifest_df['primary_label_int'].astype(int)\n",
        "\n",
        "    clip_samples = manifest_df.to_dict('records') # Convert DF rows to list of dicts\n",
        "    cfg.TOTAL_CLIPS = len(clip_samples)\n",
        "    print(f\"Loaded {cfg.TOTAL_CLIPS} clip samples from mainfest.\")\n",
        "    print(\"Sample loaded clip_info: \", clip_samples[0] if clip_samples else \"N/A\")\n",
        "else:\n",
        "  print(f\"Error: Manifest not found at {manifest_load_path}\")\n",
        "  raise FileNotFoundError(f\"Manifest missing: {manifest_load_path}\")"
      ],
      "metadata": {
        "id": "sovL4wAy_I-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training / Validation Split"
      ],
      "metadata": {
        "id": "Huti6zxLD9pY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'clip_samples' not in globals() or not clip_samples:\n",
        "  print(\"Error: clip_samples is not loaded, run the previous steps first!\")\n",
        "  raise ValueError(\"clip_samples not loaded. Can't proceed with data splitting.\")\n",
        "else:\n",
        "  clip_df_for_split = pd.DataFrame(clip_samples)\n",
        "\n",
        "  # Filter classes with only one sample - for stratification\n",
        "  label_counts = clip_df_for_split['primary_label_int'].value_counts()\n",
        "  single_sample_labels = label_counts[label_counts == 1].index.tolist()\n",
        "\n",
        "  if single_sample_labels:\n",
        "    print(f\"Found {len(single_sample_labels)} classes with only 1 precomputed sample clip.\")\n",
        "    # Classes being removed\n",
        "    clip_df_filtered = clip_df_for_split[~clip_df_for_split['primary_label_int'].isin(single_sample_labels)].copy()\n",
        "    removed_count = len(clip_df_for_split) - len(clip_df_filtered)\n",
        "    print(f\"Removed {removed_count} clips belonging to single sample classes\")\n",
        "    print(f\"Remaining clips for splitting: {len(clip_df_filtered)}\")\n",
        "  else:\n",
        "    clip_df_filtered = clip_df_for_split.copy()\n",
        "    print(\"No classes with only 1 precomputed sample clip found. No filtering applied.\")\n",
        "\n",
        "  if not clip_df_filtered.empty:\n",
        "    # Indeces of the filteredd dataframe for splitting\n",
        "    features = clip_df_filtered.index\n",
        "    labels = clip_df_filtered['primary_label_int']\n",
        "\n",
        "    try:\n",
        "      train_indeces, val_indeces = train_test_split(\n",
        "          features, # split on the df index\n",
        "          test_size=0.2,\n",
        "          random_state=cfg.SEED,\n",
        "          stratify=labels\n",
        "      )\n",
        "      # Train and validation lists of clip info dicts\n",
        "      train_clip_info = clip_df_filtered.loc[train_indeces].to_dict('records')\n",
        "      val_clip_info = clip_df_filtered.loc[val_indeces].to_dict('records')\n",
        "\n",
        "      print(f\"Training clips: {len(train_clip_info)}\")\n",
        "      print(f\"Validation clips: {len(val_clip_info)}\")\n",
        "\n",
        "      # Verify stratification - check distributions in train and val\n",
        "      train_labels_dist = pd.Series([d['primary_label_int'] for d in train_clip_info]).value_counts(normalize=True)\n",
        "      val_labels_dist = pd.Series([d['primary_label_int'] for d in val_clip_info]).value_counts(normalize=True)\n",
        "      print(\"Example class proportions: \")\n",
        "\n",
        "      if cfg.LABEL_TO_INT:\n",
        "          example_class_name = list(cfg.LABEL_TO_INT.keys())[0] # Get first class name\n",
        "          example_class_int = cfg.LABEL_TO_INT.get(example_class_name)\n",
        "          if example_class_int is not None and example_class_int not in single_sample_labels:\n",
        "              print(f\"  Class '{example_class_name}' (Index {example_class_int}):\")\n",
        "              print(f\"    Train proportion: {train_labels_dist.get(example_class_int, 0):.4f}\")\n",
        "              print(f\"    Valid proportion: {val_labels_dist.get(example_class_int, 0):.4f}\")\n",
        "          else:\n",
        "              print(f\"  Cannot verify example class '{example_class_name}' (not found or was removed).\")\n",
        "      else:\n",
        "        print(\"  Label mappings (cfg.LABEL_TO_INT) not available for verification.\")\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(f\"\\nError during stratified split: {e}\")\n",
        "        print(\"This can happen if, even after filtering, some classes have too few members for the split ratio.\")\n",
        "        print(\"Consider a smaller validation split or further data cleaning if this persists.\")\n",
        "        train_clip_info, val_clip_info = [], [] # Ensure they are empty on error\n",
        "\n"
      ],
      "metadata": {
        "id": "Do-1YF2F-F8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loaders"
      ],
      "metadata": {
        "id": "7ZZmUfrlJbpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg.BATCH_SIZE = 256 # test 256 if memory allows\n",
        "cfg.NUM_WORKERS = 2 # test 8 if config allows"
      ],
      "metadata": {
        "id": "-d8n0Wbr-F5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure time steps is correctly calculated from the precomputation parameters\n",
        "calculated_time_steps = math.floor(cfg.TARGET_SAMPLES / cfg.HOP_LENGTH) + 1\n",
        "calculated_time_steps"
      ],
      "metadata": {
        "id": "JsdkTf4D-F3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = BirdClefDataset(\n",
        "    train_clip_info,\n",
        "    cfg.PRECOMPUTED_SPECS_PATH,\n",
        "    cfg.NUM_CLASSES,\n",
        "    cfg.N_MELS,\n",
        "    calculated_time_steps,\n",
        "    augmentations=None\n",
        ")\n",
        "\n",
        "val_dataset = BirdClefDataset(\n",
        "    val_clip_info,\n",
        "    cfg.PRECOMPUTED_SPECS_PATH,\n",
        "    cfg.NUM_CLASSES,\n",
        "    cfg.N_MELS,\n",
        "    calculated_time_steps,\n",
        "    augmentations=None\n",
        ")"
      ],
      "metadata": {
        "id": "fgF3gkxg-F0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Train Dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation Dataset size: {len(val_dataset)}\")"
      ],
      "metadata": {
        "id": "5QtVNAOE-FyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    cfg.BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=cfg.NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    cfg.BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=cfg.NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    drop_last=False\n",
        ")\n",
        "\n",
        "print(f\"DataLoaders created. Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")"
      ],
      "metadata": {
        "id": "H20KbmKV-FdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Dataloaders Container ---\n",
        "class BirdDataLoaders:\n",
        "    def __init__(self, train_dl, valid_dl, device):\n",
        "        self.train = train_dl\n",
        "        self.valid = valid_dl\n",
        "        self.device = device\n",
        "    @property\n",
        "    def num_classes(self): return self.train.dataset.num_classes if self.train.dataset else 0\n",
        "\n",
        "dataloaders = BirdDataLoaders(train_loader, val_loader, device)\n",
        "print(f\"BirdDataLoaders container created. Target device: {dataloaders.device}\")"
      ],
      "metadata": {
        "id": "oyyJC4VBVMEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    spec_batch, label_batch = next(iter(train_loader))\n",
        "    print(f\"  Spectrogram batch shape: {spec_batch.shape}\")\n",
        "    print(f\"  Label batch shape: {label_batch.shape}\")\n",
        "    print(f\"  Spectrogram device: {spec_batch.device}\") # Expect 'cpu'\n",
        "    print(f\"  Spectrogram dtype: {spec_batch.dtype}\")\n",
        "    print(f\"  Label device: {label_batch.device}\")     # Expect 'cpu'\n",
        "    print(f\"  Label dtype: {label_batch.dtype}\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR fetching batch from DataLoader: {e}\")\n",
        "    print(\"Common issues: issues in Dataset __getitem__, multiprocessing setup, or data corruption.\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "id": "UKt1DnKS-Fa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model, Loss, Optimizer, Metrics"
      ],
      "metadata": {
        "id": "r6hdJOW7Pl2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_adapted_efficientnet(num_classes, pretrained=True):\n",
        "    model = timm.create_model('efficientnet_b0', pretrained=pretrained, num_classes=0) # Load with no classifier\n",
        "\n",
        "    # Modify input conv\n",
        "    original_conv_stem = model.conv_stem\n",
        "    mean_weights = original_conv_stem.weight.data.mean(dim=1, keepdim=True)\n",
        "    new_conv_stem = nn.Conv2d(\n",
        "        1, original_conv_stem.out_channels, kernel_size=original_conv_stem.kernel_size,\n",
        "        stride=original_conv_stem.stride, padding=original_conv_stem.padding,\n",
        "        bias=(original_conv_stem.bias is not None)\n",
        "    )\n",
        "    new_conv_stem.weight.data = mean_weights\n",
        "    if original_conv_stem.bias is not None:\n",
        "        new_conv_stem.bias.data = original_conv_stem.bias.data\n",
        "    model.conv_stem = new_conv_stem\n",
        "\n",
        "    # Add new classifier\n",
        "    num_in_features = model.num_features # For EfficientNet, features before classifier\n",
        "    model.classifier = nn.Linear(num_in_features, num_classes)\n",
        "\n",
        "    print(f\"EfficientNet-B0 adapted for 1-channel input and {num_classes} classes.\")\n",
        "    return model\n",
        "\n",
        "model = create_adapted_efficientnet(num_classes=cfg.NUM_CLASSES, pretrained=True)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "MbUK1Hg5-FYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_func = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=cfg.LEARNING_RATE)\n",
        "scaler = amp.GradScaler()"
      ],
      "metadata": {
        "id": "6BWMK5Ri-FWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Evaluation Metrics Function ---\n",
        "def calculate_metrics_multilabel(all_preds_logits, all_labels_true, threshold=0.5):\n",
        "    metrics = {}\n",
        "    # Ensure float32 and on CPU for sklearn\n",
        "    all_probs_np = torch.sigmoid(all_preds_logits.float()).cpu().numpy()\n",
        "    all_labels_np = all_labels_true.float().cpu().numpy()\n",
        "\n",
        "    # Sample-average accuracy\n",
        "    binary_preds = (all_probs_np >= threshold).astype(int)\n",
        "    sample_accuracy = (all_labels_np == binary_preds).mean(axis=1)\n",
        "    metrics['sample_avg_accuracy'] = np.mean(sample_accuracy)\n",
        "\n",
        "    # Macro AUC & AP\n",
        "    class_auc_scores, class_ap_scores = [], []\n",
        "    valid_auc_classes, valid_ap_classes = 0, 0\n",
        "\n",
        "    for i in range(all_labels_np.shape[1]):\n",
        "        class_labels, class_probs = all_labels_np[:, i], all_probs_np[:, i]\n",
        "        if len(np.unique(class_labels)) > 1:\n",
        "            try:\n",
        "                class_auc_scores.append(roc_auc_score(class_labels, class_probs))\n",
        "                valid_auc_classes +=1\n",
        "            except ValueError: class_auc_scores.append(np.nan)\n",
        "        else: class_auc_scores.append(np.nan)\n",
        "\n",
        "        if np.sum(class_labels) > 0 : # Need at least one positive for AP\n",
        "            try:\n",
        "                class_ap_scores.append(average_precision_score(class_labels, class_probs))\n",
        "                valid_ap_classes +=1\n",
        "            except ValueError: class_ap_scores.append(np.nan)\n",
        "        else: class_ap_scores.append(np.nan)\n",
        "\n",
        "    metrics['macro_auc'] = np.nanmean(class_auc_scores) if valid_auc_classes > 0 else 0.0\n",
        "    metrics['macro_ap'] = np.nanmean(class_ap_scores) if valid_ap_classes > 0 else 0.0\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "8BE5Vlal-FTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Validation Loops"
      ],
      "metadata": {
        "id": "spE4bMJMQX5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch_amp(model, loader, loss_func, optimizer, scaler, device):\n",
        "  model.train()\n",
        "  total_loss = 0.0\n",
        "  pbar = tqdm(loader, desc=\"Training\", leave=False)\n",
        "  for inputs , labels in pbar:\n",
        "    inputs = inputs.unsqueeze(1).to(device, non_blocking=True) # channels then move too device\n",
        "    labels = labels.to(device, non_blocking=True)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    with amp.autocast(device_type=device.type):\n",
        "      outputs = model(inputs)\n",
        "      batch_loss = loss_func(outputs, labels)\n",
        "\n",
        "    scaler.scale(batch_loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    total_loss += batch_loss.item() * inputs.size(0)\n",
        "    pbar.set_postfix(loss=f\"{batch_loss.item(): .4f}\")\n",
        "  return total_loss / len(loader.dataset)"
      ],
      "metadata": {
        "id": "q9kPX1oi-FRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#def validate_one_epoch_amp(model, loader, loss_func, device):\n",
        "#  model.eval()\n",
        "#  total_loss = 0.0\n",
        "#  all_preds_logits, all_labels_true = [], []\n",
        "#  with torch.no_grad():\n",
        "#    pbar = tqdm(loader, desc=\"Validation\", leave=False)\n",
        "#    for inputs, labels in pbar:\n",
        "#        inputs = inputs.unsqueeze(1).to(device, non_blocking=True)\n",
        "#        labels = labels.to(device, non_blocking=True)\n",
        "#\n",
        "#        with amp.autocast(device_type=device.type):\n",
        "#            outputs = model(inputs) # Logits\n",
        "#            batch_loss = loss_func(outputs, labels)\n",
        "#\n",
        "#        total_loss += batch_loss.item() * inputs.size(0)\n",
        "#        all_preds_logits.append(outputs.cpu())\n",
        "#        all_labels_true.append(labels.cpu())\n",
        "#        pbar.set_postfix(loss=f\"{batch_loss.item():.4f}\")\n",
        "#  avg_loss = total_loss / len(loader.dataset)\n",
        "#  metrics = calculate_metrics_multilabel(torch.cat(all_preds_logits), torch.cat(all_labels_true))"
      ],
      "metadata": {
        "id": "hMtNO3nvQdgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_one_epoch_amp(model, loader, loss_func, device): # Changed loss_func to criterion to match definition\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    num_samples_in_loader = len(loader.dataset) # Get total samples for accurate averaging\n",
        "\n",
        "    all_preds_logits_list = [] # Use list for append\n",
        "    all_labels_true_list = []  # Use list for append\n",
        "\n",
        "    print(f\"  Validation: Starting. Loader has {len(loader)} batches, {num_samples_in_loader} samples.\")\n",
        "\n",
        "    if num_samples_in_loader == 0:\n",
        "        print(\"  Validation: ERROR - DataLoader is empty.\")\n",
        "        # Return default values to prevent crash, but indicate error\n",
        "        return 0.0, {\"macro_auc\": 0.0, \"macro_ap\": 0.0, \"sample_avg_accuracy\": 0.0, \"error\": \"Empty DataLoader\"}\n",
        "\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            pbar = tqdm(loader, desc=\"Validation\", leave=False)\n",
        "            for batch_idx, (inputs, labels) in enumerate(pbar):\n",
        "                inputs = inputs.unsqueeze(1).to(device, non_blocking=True)\n",
        "                labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "                with amp.autocast(device_type=device.type): # Make sure device.type is correct\n",
        "                    outputs = model(inputs) # Logits\n",
        "                    batch_loss = loss_func(outputs, labels)\n",
        "\n",
        "                total_loss += batch_loss.item() * inputs.size(0)\n",
        "                all_preds_logits_list.append(outputs.cpu())\n",
        "                all_labels_true_list.append(labels.cpu())\n",
        "                pbar.set_postfix(loss=f\"{batch_loss.item():.4f}\")\n",
        "\n",
        "        # --- Check if any predictions were collected ---\n",
        "        if not all_preds_logits_list:\n",
        "            print(\"  Validation: ERROR - No predictions were collected. lists are empty.\")\n",
        "            return 0.0, {\"macro_auc\": 0.0, \"macro_ap\": 0.0, \"sample_avg_accuracy\": 0.0, \"error\": \"No predictions collected\"}\n",
        "\n",
        "        avg_loss = total_loss / num_samples_in_loader\n",
        "        print(f\"  Validation: Avg Loss calculated: {avg_loss:.4f}\")\n",
        "\n",
        "        # --- Concatenate tensors ---\n",
        "        try:\n",
        "            all_preds_tensor = torch.cat(all_preds_logits_list)\n",
        "            all_labels_tensor = torch.cat(all_labels_true_list)\n",
        "            print(f\"  Validation: Concatenated preds shape: {all_preds_tensor.shape}, labels shape: {all_labels_tensor.shape}\")\n",
        "        except Exception as e_cat:\n",
        "            print(f\"  Validation: ERROR concatenating tensors: {e_cat}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return avg_loss, {\"macro_auc\": 0.0, \"macro_ap\": 0.0, \"sample_avg_accuracy\": 0.0, \"error\": f\"Tensor concat error: {e_cat}\"}\n",
        "\n",
        "\n",
        "        # --- Calculate metrics ---\n",
        "        try:\n",
        "            print(\"  Validation: Calling calculate_metrics_multilabel...\")\n",
        "            # Ensure calculate_metrics_multilabel is defined and accessible\n",
        "            metrics = calculate_metrics_multilabel(all_preds_tensor, all_labels_tensor)\n",
        "            if metrics is None: # Explicitly check if metrics function returned None\n",
        "                print(\"  Validation: CRITICAL ERROR - calculate_metrics_multilabel returned None!\")\n",
        "                # Fallback metrics\n",
        "                metrics = {\"macro_auc\": 0.0, \"macro_ap\": 0.0, \"sample_avg_accuracy\": 0.0, \"error\": \"Metrics function returned None\"}\n",
        "            print(f\"  Validation: Metrics calculated: {metrics}\")\n",
        "        except Exception as e_metrics:\n",
        "            print(f\"  Validation: ERROR during metrics calculation: {e_metrics}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            # Fallback metrics in case of error during calculation\n",
        "            metrics = {\"macro_auc\": 0.0, \"macro_ap\": 0.0, \"sample_avg_accuracy\": 0.0, \"error\": f\"Metrics calculation error: {e_metrics}\"}\n",
        "\n",
        "        return avg_loss, metrics\n",
        "\n",
        "    except Exception as e_epoch:\n",
        "        print(f\"  Validation: UNHANDLED EXCEPTION in validate_one_epoch_amp: {e_epoch}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        # This case should ideally not be hit if inner try-excepts are good,\n",
        "        # but as a last resort, return something to prevent unpack error.\n",
        "        return 0.0, {\"macro_auc\": 0.0, \"macro_ap\": 0.0, \"sample_avg_accuracy\": 0.0, \"error\": f\"Outer validation loop error: {e_epoch}\"}"
      ],
      "metadata": {
        "id": "cfVmScSbZCUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "bU5CUWCuSaXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_training_pipeline(model, dataloaders, loss_func, optimizer, scaler, num_epochs, device):\n",
        "  start_time = time.time()\n",
        "  best_val_metric = 0.0\n",
        "  best_model_state = None\n",
        "  history = {'train_loss': [], 'val_loss': [], 'val_macro_auc': [], 'val_macro_ap': [], 'val_sample_avg_accuracy': []}\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    epoch_start_time = time.time()\n",
        "    print(f\"\\n--- Epoch {epoch+1}/{num_epochs} ---\")\n",
        "\n",
        "    train_loss = train_one_epoch_amp(model, dataloaders.train, loss_func, optimizer, scaler, device)\n",
        "    history['train_loss'].append(train_loss)\n",
        "    print(f\"Epoch {epoch+1} Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "    val_loss, metrics = validate_one_epoch_amp(model, dataloaders.valid, loss_func, device)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    for key, val in metrics.items(): history[f'val_{key}'].append(val) # Dynamic metric logging\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Val Loss : {val_loss:.4f}\")\n",
        "    for k, v in metrics.items(): print(f\"  Val {k}: {v:.4f}\")\n",
        "\n",
        "    current_val_metric = metrics.get('macro_auc', 0.0)\n",
        "    if current_val_metric > best_val_metric:\n",
        "        best_val_metric = current_val_metric\n",
        "        best_model_state = copy.deepcopy(model.state_dict())\n",
        "        print(f\"*** New best Macro AUC: {best_val_metric:.4f}. Saving model state. ***\")\n",
        "        # torch.save(best_model_state, f'/content/best_model_epoch_{epoch+1}.pth') # Optional: save to file\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Time: {time.time() - epoch_start_time:.2f}s\")\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "  total_time = time.time() - start_time\n",
        "  print(f\"\\n--- Training Finished ---\")\n",
        "  print(f\"Total Time: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
        "  print(f\"Best Val Macro AUC: {best_val_metric:.4f}\")\n",
        "  if best_model_state: model.load_state_dict(best_model_state)\n",
        "  return model, history"
      ],
      "metadata": {
        "id": "a-3iE6rxQdeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nStarting training: {cfg.EPOCHS} epochs, BS={cfg.BATCH_SIZE}, Workers={cfg.NUM_WORKERS}\")\n",
        "\n",
        "if 'dataloaders' in globals() and dataloaders is not None:\n",
        "  trained_model, training_history = run_training_pipeline(\n",
        "      model,\n",
        "      dataloaders,\n",
        "      loss_func,\n",
        "      optimizer,\n",
        "      scaler,\n",
        "      cfg.EPOCHS,\n",
        "      device\n",
        "  )\n",
        "else:\n",
        "  print(\"Error: Dataloaders not initialized, re-run the pipeline.\")\n",
        "\n",
        "# --- Plot Results ---\n",
        "if 'training_history' in globals():\n",
        "    epochs_range = range(1, len(training_history['train_loss']) + 1)\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(epochs_range, training_history['train_loss'], 'bo-', label='Training Loss')\n",
        "    plt.plot(epochs_range, training_history['val_loss'], 'ro-', label='Validation Loss')\n",
        "    plt.title('Loss'); plt.xlabel('Epochs'); plt.ylabel('Loss'); plt.legend(); plt.grid(True)\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(epochs_range, training_history['val_macro_auc'], 'go-', label='Val Macro AUC')\n",
        "    plt.title('Macro AUC'); plt.xlabel('Epochs'); plt.ylabel('AUC'); plt.legend(); plt.grid(True)\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(epochs_range, training_history['val_sample_avg_accuracy'], 'mo-', label='Val Sample Avg Accuracy')\n",
        "    plt.title('Sample Avg Accuracy'); plt.xlabel('Epochs'); plt.ylabel('Accuracy'); plt.legend(); plt.grid(True)\n",
        "    plt.tight_layout(); plt.show()"
      ],
      "metadata": {
        "id": "ctKopoz3Qdbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BO_Itd7eU8qO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}