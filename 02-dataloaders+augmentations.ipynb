{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":91844,"databundleVersionId":11361821,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Move imports to utility\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os, gc, random \nfrom pathlib import Path\nfrom tqdm.notebook import tqdm\nimport IPython.display as ipd\nfrom IPython.display import display, clear_output\nimport ipywidgets as widgets\n\nimport librosa\nimport librosa.display\nimport soundfile as sf\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchaudio.transforms as T\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-29T06:41:16.644144Z","iopub.execute_input":"2025-04-29T06:41:16.644919Z","iopub.status.idle":"2025-04-29T06:41:16.651235Z","shell.execute_reply.started":"2025-04-29T06:41:16.644888Z","shell.execute_reply":"2025-04-29T06:41:16.650467Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"NOTE\n- See if imports, along with preprocessingg and augmentation funcs can easily be moved to the utility folder\n","metadata":{}},{"cell_type":"code","source":"class Config:\n    SEED = 42\n    SAMPLE_RATE = 32000\n    DATA_PATH = Path(\"/kaggle/input/birdclef-2025\")\n    # Spectrogram Params\n    N_MELS = 128\n    N_FFT = 2048\n    HOP_LENGTH = 512\n    FMIN = 50\n    FMAX = 14000\n    # Clip params (setting for 5s to align with submission policy)\n    TARGET_DURATION_S = 5\n    TARGET_SAMPLES = TARGET_DURATION_S * SAMPLE_RATE\n    # Path for training audio\n    TRAIN_AUDIO_PATH = DATA_PATH/\"train_audio\"\n    TRAIN_METADATA_PATH = DATA_PATH/\"train.csv\"\n    TAXONOMY_PATH = DATA_PATH/\"taxonomy.csv\"\n    # additional paths here\n    #TRAIN_SOUNDSCAPES_PATH = DATA_PATH/\"train_soundscapes\"\n    #TEST_SOUNDSCAPES_PATH = DATA_PATH/\"test_soundscapes\"    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T06:41:18.194203Z","iopub.execute_input":"2025-04-29T06:41:18.194901Z","iopub.status.idle":"2025-04-29T06:41:18.198821Z","shell.execute_reply.started":"2025-04-29T06:41:18.194876Z","shell.execute_reply":"2025-04-29T06:41:18.198162Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Function to seed everything to ensure reproducibility\ndef seed_everything(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False # Change to true if input sizes are kept constant","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T06:41:18.481596Z","iopub.execute_input":"2025-04-29T06:41:18.481836Z","iopub.status.idle":"2025-04-29T06:41:18.486130Z","shell.execute_reply.started":"2025-04-29T06:41:18.481818Z","shell.execute_reply":"2025-04-29T06:41:18.485329Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Setup \ncfg = Config()\nseed_everything(cfg.SEED)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T06:41:18.734803Z","iopub.execute_input":"2025-04-29T06:41:18.735043Z","iopub.status.idle":"2025-04-29T06:41:18.740076Z","shell.execute_reply.started":"2025-04-29T06:41:18.735024Z","shell.execute_reply":"2025-04-29T06:41:18.739358Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Loading Taxonomy data for context\nif cfg.TAXONOMY_PATH.exists():\n    taxonomy_df = pd.read_csv(cfg.TAXONOMY_PATH)\n    print(\"Taxonomy data loaded\")\n    print(taxonomy_df.head())\n    print(\"\\nClass distribution across taxa: \")\n    print(taxonomy_df['class_name'].value_counts())\nelse:\n    print(f\"File not found at {cfg.TAXONOMY_PATH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T06:41:19.526603Z","iopub.execute_input":"2025-04-29T06:41:19.527331Z","iopub.status.idle":"2025-04-29T06:41:19.537823Z","shell.execute_reply.started":"2025-04-29T06:41:19.527307Z","shell.execute_reply":"2025-04-29T06:41:19.537169Z"}},"outputs":[{"name":"stdout","text":"Taxonomy data loaded\n  primary_label  inat_taxon_id               scientific_name  \\\n0       1139490        1139490          Ragoniella pulchella   \n1       1192948        1192948         Oxyprora surinamensis   \n2       1194042        1194042           Copiphora colombiae   \n3        126247         126247       Leptodactylus insularum   \n4       1346504        1346504  Neoconocephalus brachypterus   \n\n                    common_name class_name  \n0          Ragoniella pulchella    Insecta  \n1         Oxyprora surinamensis    Insecta  \n2           Copiphora colombiae    Insecta  \n3        Spotted Foam-nest Frog   Amphibia  \n4  Neoconocephalus brachypterus    Insecta  \n\nClass distribution across taxa: \nclass_name\nAves        146\nAmphibia     34\nInsecta      17\nMammalia      9\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"Move to the utility file once this is tested","metadata":{}},{"cell_type":"code","source":"mel_spectrogram_tfms = T.MelSpectrogram(\n    sample_rate = cfg.SAMPLE_RATE,\n    n_fft = cfg.N_FFT,\n    hop_length = cfg.HOP_LENGTH,\n    n_mels = cfg.N_MELS,\n    f_min = cfg.FMIN,\n    f_max = cfg.FMAX\n).to(device) # transforms are moved to the GPU","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T06:41:21.485403Z","iopub.execute_input":"2025-04-29T06:41:21.486043Z","iopub.status.idle":"2025-04-29T06:41:21.494021Z","shell.execute_reply.started":"2025-04-29T06:41:21.486017Z","shell.execute_reply":"2025-04-29T06:41:21.493254Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Connverty power spec to DB\namp_to_db_tfms = T.AmplitudeToDB(stype='power', top_db=80).to(device) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T06:41:50.911129Z","iopub.execute_input":"2025-04-29T06:41:50.911449Z","iopub.status.idle":"2025-04-29T06:41:50.916173Z","shell.execute_reply.started":"2025-04-29T06:41:50.911425Z","shell.execute_reply":"2025-04-29T06:41:50.915430Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Another one for the utility file\n# Then clean up\ndef pre_process_audio_file(file_path: Path, target_sr=cfg.SAMPLE_RATE,\n                          target_samples=cfg.TARGET_SAMPLES):\n    \"\"\"Loads an audio file, resamples, splits into 5s non-overlapping clips,\n    and converts each clip to a Mel spectrogram tensor.\"\"\"\n    spectrograms = []\n    try: \n        waveform, sr = torchaudio.load(filepath) #return tensor [channels, time]\n        # Resample if needed\n        if sr != target_sr:\n            resampler = T.Resampler(sr, target_sr).to(waveform.device)\n            waveform = resampler(waveform)\n        # Convert to mono if stereo\n        if waveform.shape[0] > 1:\n            waveform = torch.mean(waveform, dim=0, keepdim=True)\n        # Ensure waveform is on the correct device\n        waveform = waveform.to(device)\n        # Calculate number of full clips\n        num_samples = waveform.shape[1]\n        num_full_clips = num_samples // target_samples\n\n        for i in range(num_full_clips):\n            start_sample = i * target_samples\n            end_sample = sample_sample + target_samples\n            clip = waveform[:, start_sample:end_sample] # Keep channel dim for tfms\n            # Generate Mel spectrogram\n            mel_spec = mel_spectrogram_transform(clip) #Output: [channel, n_mels, time]\n            # Convert to DB scale\n            mel_spec_db = amplitude_to_db_transform(mel_spec)# Output [channel, n_mels, time]\n            # Squeeze channel dims -> [n_mels, time]\n            spectrograms.append(mel_spec_db.squeeze(0))\n\n        # If we wanted to use the remainder for training, we could pad it here:\n        # remaining_samples = num_samples % target_samples\n        # if remaining_samples > 0 and end_behavior=='pad': # Example\n        #    last_clip = waveform[:, num_full_clips*target_samples:]\n        #    padding = target_samples - last_clip.shape[1]\n        #    last_clip_padded = torch.nn.functional.pad(last_clip, (0, padding))\n    except Exception as e:\n        print(f\"Error processing {file_path.name}: {e}\")\n        # return empty list on error\n        return []\n    return spectrograms","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test on some samples\nif 'train_df' in globals","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}