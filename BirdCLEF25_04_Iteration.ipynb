{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1EM51knAqkgdgKR_LEsvL0B8NRH_-0_to",
      "authorship_tag": "ABX9TyNcI+et0MjmJAAPh/DFOFfu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bachaudhry/kaggle_birdCLEF_25/blob/main/BirdCLEF25_04_Iteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BirdCLEF 25 - Iterating From Baseline"
      ],
      "metadata": {
        "id": "MPhonWFFUZEg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Logging in Gsheets\n",
        "*   Implementing SpecAugment and MixUp/CutMix on Spectrograms\n",
        "*   Learning Rate Scheduler and early stopping\n",
        "\n"
      ],
      "metadata": {
        "id": "ESMMicCb-ymW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.environ[\"KAGGLE_CONFIG_DIR\"] = \"/content/drive/MyDrive/Kaggle\""
      ],
      "metadata": {
        "id": "6jOrk6FGUW24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle\n",
        "!pip install gspread gspread-dataframe oauth2client"
      ],
      "metadata": {
        "id": "QKPHMfMtgmSK",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sys, gc, random, math, time, copy , zipfile, tarfile, shutil, subprocess, json\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "import IPython.display as ipd\n",
        "from IPython.display import display, clear_output\n",
        "import ipywidgets as widgets\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "import soundfile as sf\n",
        "\n",
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.multiprocessing as mp\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.amp as amp\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, average_precision_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# For logging experiment results to GSheets.\n",
        "import time, copy, gspread\n",
        "from gspread_dataframe import set_with_dataframe\n",
        "from google.colab import auth\n",
        "from google.auth import default as GoogleAuthDefault"
      ],
      "metadata": {
        "id": "F0dtrnjeUFrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append('/content/drive/MyDrive/Kaggle/Bird_CLEF25/utils')\n",
        "from utils import Config, BirdClefDataset, create_target_tensor, seed_everything, process_gzipped"
      ],
      "metadata": {
        "id": "4VY8jZA4fZWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = Config()\n",
        "# Path to original train.csv, audio and metadata\n",
        "cfg.BASE_DATA_PATH = Path(\"/content/drive/MyDrive/Kaggle/Bird_CLEF25/data/birdclef-2025\")\n",
        "# Path to npy files\n",
        "cfg.PRECOMPUTED_SPECS_PATH = Path(\"/content/drive/MyDrive/Kaggle/Bird_CLEF25/data/precomputed-specs-np-zipped\")\n",
        "# Path to local specs\n",
        "#cfg.LOCAL_SPECS_PATH = Path(\"/content/precomputed_spectrograms\")\n",
        "# Training meta data\n",
        "cfg.TRAIN_METADATA_PATH = Path(\"/content/drive/MyDrive/Kaggle/Bird_CLEF25/data/birdclef-2025/train.csv\")"
      ],
      "metadata": {
        "id": "LmOAPiLPfZTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Set Device & Seed ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "seed_everything(cfg.SEED)"
      ],
      "metadata": {
        "id": "QYdzB_OafZQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if cfg.NUM_WORKERS > 0:\n",
        "    try:\n",
        "        current_context = mp.get_context(None)\n",
        "        if not isinstance(current_context, mp.SpawnContext):\n",
        "             mp.set_start_method('spawn', force=True)\n",
        "             print(\"Set multiprocessing start method to 'spawn'.\")\n",
        "        else:\n",
        "             print(\"Multiprocessing start method already set to 'spawn'.\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Could not set start method (might be already set or first run): {e}\")"
      ],
      "metadata": {
        "id": "ZRsuayJcfZOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load Metadata ---\n",
        "if not cfg.TRAIN_METADATA_PATH.exists():\n",
        "    print(f\"ERROR: Metadata file not found at {cfg.TRAIN_METADATA_PATH}\")\n",
        "    # Stop execution or handle\n",
        "else:\n",
        "    train_df = pd.read_csv(cfg.TRAIN_METADATA_PATH)\n",
        "    print(f\"Train metadata loaded. Shape: {train_df.shape}\")"
      ],
      "metadata": {
        "id": "RjDZIs1ZfZL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download zipped folder and extract to local # One time per setup\n",
        "input_path = \"/content/drive/MyDrive/Kaggle/Bird_CLEF25/data/precomputed-specs-np-zipped\"\n",
        "local_extract = process_gzipped(input_path)"
      ],
      "metadata": {
        "id": "ynDHKvxkJMqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update precomputed specs path\n",
        "#cfg.PRECOMPUTED_SPECS_PATH = cfg.LOCAL_SPECS_PATH\n",
        "# Run check\n",
        "local_specs_path = Path(\"/content/temp_data/extracted/kaggle/working/precomputed_specs_np\")\n",
        "all_precomputed_files = list(local_specs_path.glob(\"*.npy\"))\n",
        "print(f\"Found {len(all_precomputed_files)} precomputed .npy files.\")"
      ],
      "metadata": {
        "id": "kLlg8hWRfZJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg.PRECOMPUTED_SPECS_PATH = local_specs_path"
      ],
      "metadata": {
        "id": "CQFu-UaLk3Lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create label mappings\n",
        "unique_labels = sorted(train_df['primary_label'].unique())\n",
        "cfg.NUM_CLASSES = len(unique_labels)\n",
        "cfg.LABEL_TO_INT = {label: i for i, label in enumerate(unique_labels)}\n",
        "cfg.INT_TO_LABEL = {i: label for label, i in cfg.LABEL_TO_INT.items()}\n",
        "train_df['primary_label_int'] = train_df['primary_label'].map(cfg.LABEL_TO_INT)\n",
        "print(f\"{cfg.NUM_CLASSES} unique classes found.\")"
      ],
      "metadata": {
        "id": "YOOBGzhUfZEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "kCeUnrD6uTBB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This next step involves the creation of a manifest file to make the process of ensuring that preprocessed spectrograms are mapped to their respective originals in the train DF metadata file.\n",
        "\n",
        "If done correctly this cell will only need to be run once so that a `manifest.csv` file is created and stored in Google Drive. Afterwards, this step can be commented out and will be replaced with a new cell to simply load the manifest file from source."
      ],
      "metadata": {
        "id": "K6nQOKw083w7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "G1dHnCfpuWDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load manifest data\n",
        "manifest_load_path = Path(\"/content/drive/MyDrive/Kaggle/Bird_CLEF25/data/manifest.csv\")\n",
        "if manifest_load_path.exists():\n",
        "    manifest_df = pd.read_csv(manifest_load_path)\n",
        "    print(f\"Manifest loaded from {manifest_load_path}\")\n",
        "    print(manifest_df.head(15))\n",
        "\n",
        "    # Ensure correct data types\n",
        "    manifest_df['clip_index'] = manifest_df['clip_index'].astype(int)\n",
        "    manifest_df['primary_label_int'] = manifest_df['primary_label_int'].astype(int)\n",
        "\n",
        "    clip_samples = manifest_df.to_dict('records') # Convert DF rows to list of dicts\n",
        "    cfg.TOTAL_CLIPS = len(clip_samples)\n",
        "    print(f\"Loaded {cfg.TOTAL_CLIPS} clip samples from mainfest.\")\n",
        "    print(\"Sample loaded clip_info: \", clip_samples[0] if clip_samples else \"N/A\")\n",
        "else:\n",
        "  print(f\"Error: Manifest not found at {manifest_load_path}\")\n",
        "  raise FileNotFoundError(f\"Manifest missing: {manifest_load_path}\")"
      ],
      "metadata": {
        "id": "sovL4wAy_I-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training / Validation Split"
      ],
      "metadata": {
        "id": "Huti6zxLD9pY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'clip_samples' not in globals() or not clip_samples:\n",
        "  print(\"Error: clip_samples is not loaded, run the previous steps first!\")\n",
        "  raise ValueError(\"clip_samples not loaded. Can't proceed with data splitting.\")\n",
        "else:\n",
        "  clip_df_for_split = pd.DataFrame(clip_samples)\n",
        "\n",
        "  # Filter classes with only one sample - for stratification\n",
        "  label_counts = clip_df_for_split['primary_label_int'].value_counts()\n",
        "  single_sample_labels = label_counts[label_counts == 1].index.tolist()\n",
        "\n",
        "  if single_sample_labels:\n",
        "    print(f\"Found {len(single_sample_labels)} classes with only 1 precomputed sample clip.\")\n",
        "    # Classes being removed\n",
        "    clip_df_filtered = clip_df_for_split[~clip_df_for_split['primary_label_int'].isin(single_sample_labels)].copy()\n",
        "    removed_count = len(clip_df_for_split) - len(clip_df_filtered)\n",
        "    print(f\"Removed {removed_count} clips belonging to single sample classes\")\n",
        "    print(f\"Remaining clips for splitting: {len(clip_df_filtered)}\")\n",
        "  else:\n",
        "    clip_df_filtered = clip_df_for_split.copy()\n",
        "    print(\"No classes with only 1 precomputed sample clip found. No filtering applied.\")\n",
        "\n",
        "  if not clip_df_filtered.empty:\n",
        "    # Indeces of the filteredd dataframe for splitting\n",
        "    features = clip_df_filtered.index\n",
        "    labels = clip_df_filtered['primary_label_int']\n",
        "\n",
        "    try:\n",
        "      train_indeces, val_indeces = train_test_split(\n",
        "          features, # split on the df index\n",
        "          test_size=0.2,\n",
        "          random_state=cfg.SEED,\n",
        "          stratify=labels\n",
        "      )\n",
        "      # Train and validation lists of clip info dicts\n",
        "      train_clip_info = clip_df_filtered.loc[train_indeces].to_dict('records')\n",
        "      val_clip_info = clip_df_filtered.loc[val_indeces].to_dict('records')\n",
        "\n",
        "      print(f\"Training clips: {len(train_clip_info)}\")\n",
        "      print(f\"Validation clips: {len(val_clip_info)}\")\n",
        "\n",
        "      # Verify stratification - check distributions in train and val\n",
        "      train_labels_dist = pd.Series([d['primary_label_int'] for d in train_clip_info]).value_counts(normalize=True)\n",
        "      val_labels_dist = pd.Series([d['primary_label_int'] for d in val_clip_info]).value_counts(normalize=True)\n",
        "      print(\"Example class proportions: \")\n",
        "\n",
        "      if cfg.LABEL_TO_INT:\n",
        "          example_class_name = list(cfg.LABEL_TO_INT.keys())[0] # Get first class name\n",
        "          example_class_int = cfg.LABEL_TO_INT.get(example_class_name)\n",
        "          if example_class_int is not None and example_class_int not in single_sample_labels:\n",
        "              print(f\"  Class '{example_class_name}' (Index {example_class_int}):\")\n",
        "              print(f\"    Train proportion: {train_labels_dist.get(example_class_int, 0):.4f}\")\n",
        "              print(f\"    Valid proportion: {val_labels_dist.get(example_class_int, 0):.4f}\")\n",
        "          else:\n",
        "              print(f\"  Cannot verify example class '{example_class_name}' (not found or was removed).\")\n",
        "      else:\n",
        "        print(\"  Label mappings (cfg.LABEL_TO_INT) not available for verification.\")\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(f\"\\nError during stratified split: {e}\")\n",
        "        print(\"This can happen if, even after filtering, some classes have too few members for the split ratio.\")\n",
        "        print(\"Consider a smaller validation split or further data cleaning if this persists.\")\n",
        "        train_clip_info, val_clip_info = [], [] # Ensure they are empty on error\n",
        "\n"
      ],
      "metadata": {
        "id": "Do-1YF2F-F8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loaders"
      ],
      "metadata": {
        "id": "7ZZmUfrlJbpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "y_7r-n7fXZQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg.BATCH_SIZE = 512 # test 256 if memory allows\n",
        "cfg.NUM_WORKERS = 1 # test 8 if config allows\n",
        "\n",
        "# For Specaugment - tune as needed\n",
        "FREQ_MASK_PARAM = 12\n",
        "TIME_MASK_PARAM = 25"
      ],
      "metadata": {
        "id": "-d8n0Wbr-F5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure time steps is correctly calculated from the precomputation parameters\n",
        "calculated_time_steps = math.floor(cfg.TARGET_SAMPLES / cfg.HOP_LENGTH) + 1\n",
        "calculated_time_steps"
      ],
      "metadata": {
        "id": "JsdkTf4D-F3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = BirdClefDataset(\n",
        "    train_clip_info,\n",
        "    cfg.PRECOMPUTED_SPECS_PATH,\n",
        "    cfg.NUM_CLASSES,\n",
        "    cfg.N_MELS,\n",
        "    calculated_time_steps,\n",
        "    augmentations=None, # For custom augmentations\n",
        "    use_spec_augment=True, # enable specaugment for training set\n",
        "    freq_mask_param=FREQ_MASK_PARAM,\n",
        "    time_mask_param=TIME_MASK_PARAM\n",
        ")\n",
        "\n",
        "val_dataset = BirdClefDataset(\n",
        "    val_clip_info,\n",
        "    cfg.PRECOMPUTED_SPECS_PATH,\n",
        "    cfg.NUM_CLASSES,\n",
        "    cfg.N_MELS,\n",
        "    calculated_time_steps,\n",
        "    augmentations=None,\n",
        "    use_spec_augment=False\n",
        ")"
      ],
      "metadata": {
        "id": "fgF3gkxg-F0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Train Dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation Dataset size: {len(val_dataset)}\")"
      ],
      "metadata": {
        "id": "5QtVNAOE-FyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    cfg.BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=cfg.NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    cfg.BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=cfg.NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    drop_last=False\n",
        ")\n",
        "\n",
        "print(f\"DataLoaders created. Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")"
      ],
      "metadata": {
        "id": "H20KbmKV-FdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Dataloaders Container ---\n",
        "class BirdDataLoaders:\n",
        "    def __init__(self, train_dl, valid_dl, device):\n",
        "        self.train = train_dl\n",
        "        self.valid = valid_dl\n",
        "        self.device = device\n",
        "    @property\n",
        "    def num_classes(self): return self.train.dataset.num_classes if self.train.dataset else 0\n",
        "\n",
        "dataloaders = BirdDataLoaders(train_loader, val_loader, device)\n",
        "print(f\"BirdDataLoaders container created. Target device: {dataloaders.device}\")"
      ],
      "metadata": {
        "id": "oyyJC4VBVMEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    spec_batch, label_batch = next(iter(train_loader))\n",
        "    print(f\"  Spectrogram batch shape: {spec_batch.shape}\")\n",
        "    print(f\"  Label batch shape: {label_batch.shape}\")\n",
        "    print(f\"  Spectrogram device: {spec_batch.device}\") # Expect 'cpu'\n",
        "    print(f\"  Spectrogram dtype: {spec_batch.dtype}\")\n",
        "    print(f\"  Label device: {label_batch.device}\")     # Expect 'cpu'\n",
        "    print(f\"  Label dtype: {label_batch.dtype}\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR fetching batch from DataLoader: {e}\")\n",
        "    print(\"Common issues: issues in Dataset __getitem__, multiprocessing setup, or data corruption.\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "id": "UKt1DnKS-Fa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model, Loss, Optimizer, Metrics"
      ],
      "metadata": {
        "id": "r6hdJOW7Pl2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_adapted_efficientnet(num_classes, pretrained=True):\n",
        "    model = timm.create_model('efficientnet_b0', pretrained=pretrained, num_classes=0) # Load with no classifier\n",
        "\n",
        "    # Modify input conv\n",
        "    original_conv_stem = model.conv_stem\n",
        "    mean_weights = original_conv_stem.weight.data.mean(dim=1, keepdim=True)\n",
        "    new_conv_stem = nn.Conv2d(\n",
        "        1, original_conv_stem.out_channels, kernel_size=original_conv_stem.kernel_size,\n",
        "        stride=original_conv_stem.stride, padding=original_conv_stem.padding,\n",
        "        bias=(original_conv_stem.bias is not None)\n",
        "    )\n",
        "    new_conv_stem.weight.data = mean_weights\n",
        "    if original_conv_stem.bias is not None:\n",
        "        new_conv_stem.bias.data = original_conv_stem.bias.data\n",
        "    model.conv_stem = new_conv_stem\n",
        "\n",
        "    # Add new classifier\n",
        "    num_in_features = model.num_features # For EfficientNet, features before classifier\n",
        "    model.classifier = nn.Linear(num_in_features, num_classes)\n",
        "\n",
        "    print(f\"EfficientNet-B0 adapted for 1-channel input and {num_classes} classes.\")\n",
        "    return model\n",
        "\n",
        "model = create_adapted_efficientnet(num_classes=cfg.NUM_CLASSES, pretrained=True)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "MbUK1Hg5-FYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_func = nn.BCEWithLogitsLoss()\n",
        "cfg.LEARNING_RATE = 1e-3\n",
        "optimizer = optim.AdamW(model.parameters(), lr=cfg.LEARNING_RATE)\n",
        "scaler = amp.GradScaler()\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max',\n",
        "                                                       factor=0.2, patience=5,\n",
        "                                                       verbose=True)"
      ],
      "metadata": {
        "id": "6BWMK5Ri-FWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Evaluation Metrics Function ---\n",
        "def calculate_metrics_multilabel(all_preds_logits, all_labels_true, threshold=0.5):\n",
        "    metrics = {}\n",
        "    # Ensure float32 and on CPU for sklearn\n",
        "    all_probs_np = torch.sigmoid(all_preds_logits.float()).cpu().numpy()\n",
        "    all_labels_np = all_labels_true.float().cpu().numpy()\n",
        "\n",
        "    # Sample-average accuracy\n",
        "    binary_preds = (all_probs_np >= threshold).astype(int)\n",
        "    sample_accuracy = (all_labels_np == binary_preds).mean(axis=1)\n",
        "    metrics['sample_avg_accuracy'] = np.mean(sample_accuracy)\n",
        "\n",
        "    # Macro AUC & AP\n",
        "    class_auc_scores, class_ap_scores = [], []\n",
        "    valid_auc_classes, valid_ap_classes = 0, 0\n",
        "\n",
        "    for i in range(all_labels_np.shape[1]):\n",
        "        class_labels, class_probs = all_labels_np[:, i], all_probs_np[:, i]\n",
        "        if len(np.unique(class_labels)) > 1:\n",
        "            try:\n",
        "                class_auc_scores.append(roc_auc_score(class_labels, class_probs))\n",
        "                valid_auc_classes +=1\n",
        "            except ValueError: class_auc_scores.append(np.nan)\n",
        "        else: class_auc_scores.append(np.nan)\n",
        "\n",
        "        if np.sum(class_labels) > 0 : # Need at least one positive for AP\n",
        "            try:\n",
        "                class_ap_scores.append(average_precision_score(class_labels, class_probs))\n",
        "                valid_ap_classes +=1\n",
        "            except ValueError: class_ap_scores.append(np.nan)\n",
        "        else: class_ap_scores.append(np.nan)\n",
        "\n",
        "    metrics['macro_auc'] = np.nanmean(class_auc_scores) if valid_auc_classes > 0 else 0.0\n",
        "    metrics['macro_ap'] = np.nanmean(class_ap_scores) if valid_ap_classes > 0 else 0.0\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "8BE5Vlal-FTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Validation Loops"
      ],
      "metadata": {
        "id": "spE4bMJMQX5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch_amp(model, loader, loss_func, optimizer, scaler, device):\n",
        "  model.train()\n",
        "  total_loss = 0.0\n",
        "  pbar = tqdm(loader, desc=\"Training\", leave=False)\n",
        "  for inputs , labels in pbar:\n",
        "    inputs = inputs.unsqueeze(1).to(device, non_blocking=True) # channels then move too device\n",
        "    labels = labels.to(device, non_blocking=True)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    with amp.autocast(device_type=device.type):\n",
        "      outputs = model(inputs)\n",
        "      batch_loss = loss_func(outputs, labels)\n",
        "\n",
        "    scaler.scale(batch_loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    total_loss += batch_loss.item() * inputs.size(0)\n",
        "    pbar.set_postfix(loss=f\"{batch_loss.item(): .4f}\")\n",
        "  return total_loss / len(loader.dataset)"
      ],
      "metadata": {
        "id": "q9kPX1oi-FRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#def validate_one_epoch_amp(model, loader, loss_func, device):\n",
        "#  model.eval()\n",
        "#  total_loss = 0.0\n",
        "#  all_preds_logits, all_labels_true = [], []\n",
        "#  with torch.no_grad():\n",
        "#    pbar = tqdm(loader, desc=\"Validation\", leave=False)\n",
        "#    for inputs, labels in pbar:\n",
        "#        inputs = inputs.unsqueeze(1).to(device, non_blocking=True)\n",
        "#        labels = labels.to(device, non_blocking=True)\n",
        "#\n",
        "#        with amp.autocast(device_type=device.type):\n",
        "#            outputs = model(inputs) # Logits\n",
        "#            batch_loss = loss_func(outputs, labels)\n",
        "#\n",
        "#        total_loss += batch_loss.item() * inputs.size(0)\n",
        "#        all_preds_logits.append(outputs.cpu())\n",
        "#        all_labels_true.append(labels.cpu())\n",
        "#        pbar.set_postfix(loss=f\"{batch_loss.item():.4f}\")\n",
        "#  avg_loss = total_loss / len(loader.dataset)\n",
        "#  metrics = calculate_metrics_multilabel(torch.cat(all_preds_logits), torch.cat(all_labels_true))"
      ],
      "metadata": {
        "id": "hMtNO3nvQdgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_one_epoch_amp(model, loader, loss_func, device): # Changed loss_func to criterion to match definition\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    num_samples_in_loader = len(loader.dataset) # Get total samples for accurate averaging\n",
        "\n",
        "    all_preds_logits_list = [] # Use list for append\n",
        "    all_labels_true_list = []  # Use list for append\n",
        "\n",
        "    print(f\"  Validation: Starting. Loader has {len(loader)} batches, {num_samples_in_loader} samples.\")\n",
        "\n",
        "    if num_samples_in_loader == 0:\n",
        "        print(\"  Validation: ERROR - DataLoader is empty.\")\n",
        "        # Return default values to prevent crash, but indicate error\n",
        "        return 0.0, {\"macro_auc\": 0.0, \"macro_ap\": 0.0, \"sample_avg_accuracy\": 0.0, \"error\": \"Empty DataLoader\"}\n",
        "\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            pbar = tqdm(loader, desc=\"Validation\", leave=False)\n",
        "            for batch_idx, (inputs, labels) in enumerate(pbar):\n",
        "                inputs = inputs.unsqueeze(1).to(device, non_blocking=True)\n",
        "                labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "                with amp.autocast(device_type=device.type): # Make sure device.type is correct\n",
        "                    outputs = model(inputs) # Logits\n",
        "                    batch_loss = loss_func(outputs, labels)\n",
        "\n",
        "                total_loss += batch_loss.item() * inputs.size(0)\n",
        "                all_preds_logits_list.append(outputs.cpu())\n",
        "                all_labels_true_list.append(labels.cpu())\n",
        "                pbar.set_postfix(loss=f\"{batch_loss.item():.4f}\")\n",
        "\n",
        "        # --- Check if any predictions were collected ---\n",
        "        if not all_preds_logits_list:\n",
        "            print(\"  Validation: ERROR - No predictions were collected. lists are empty.\")\n",
        "            return 0.0, {\"macro_auc\": 0.0, \"macro_ap\": 0.0, \"sample_avg_accuracy\": 0.0, \"error\": \"No predictions collected\"}\n",
        "\n",
        "        avg_loss = total_loss / num_samples_in_loader\n",
        "        print(f\"  Validation: Avg Loss calculated: {avg_loss:.4f}\")\n",
        "\n",
        "        # --- Concatenate tensors ---\n",
        "        try:\n",
        "            all_preds_tensor = torch.cat(all_preds_logits_list)\n",
        "            all_labels_tensor = torch.cat(all_labels_true_list)\n",
        "            print(f\"  Validation: Concatenated preds shape: {all_preds_tensor.shape}, labels shape: {all_labels_tensor.shape}\")\n",
        "        except Exception as e_cat:\n",
        "            print(f\"  Validation: ERROR concatenating tensors: {e_cat}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return avg_loss, {\"macro_auc\": 0.0, \"macro_ap\": 0.0, \"sample_avg_accuracy\": 0.0, \"error\": f\"Tensor concat error: {e_cat}\"}\n",
        "\n",
        "\n",
        "        # --- Calculate metrics ---\n",
        "        try:\n",
        "            print(\"  Validation: Calling calculate_metrics_multilabel...\")\n",
        "            # Ensure calculate_metrics_multilabel is defined and accessible\n",
        "            metrics = calculate_metrics_multilabel(all_preds_tensor, all_labels_tensor)\n",
        "            if metrics is None: # Explicitly check if metrics function returned None\n",
        "                print(\"  Validation: CRITICAL ERROR - calculate_metrics_multilabel returned None!\")\n",
        "                # Fallback metrics\n",
        "                metrics = {\"macro_auc\": 0.0, \"macro_ap\": 0.0, \"sample_avg_accuracy\": 0.0, \"error\": \"Metrics function returned None\"}\n",
        "            print(f\"  Validation: Metrics calculated: {metrics}\")\n",
        "        except Exception as e_metrics:\n",
        "            print(f\"  Validation: ERROR during metrics calculation: {e_metrics}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            # Fallback metrics in case of error during calculation\n",
        "            metrics = {\"macro_auc\": 0.0, \"macro_ap\": 0.0, \"sample_avg_accuracy\": 0.0, \"error\": f\"Metrics calculation error: {e_metrics}\"}\n",
        "\n",
        "        return avg_loss, metrics\n",
        "\n",
        "    except Exception as e_epoch:\n",
        "        print(f\"  Validation: UNHANDLED EXCEPTION in validate_one_epoch_amp: {e_epoch}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        # This case should ideally not be hit if inner try-excepts are good,\n",
        "        # but as a last resort, return something to prevent unpack error.\n",
        "        return 0.0, {\"macro_auc\": 0.0, \"macro_ap\": 0.0, \"sample_avg_accuracy\": 0.0, \"error\": f\"Outer validation loop error: {e_epoch}\"}"
      ],
      "metadata": {
        "id": "cfVmScSbZCUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and Log Results"
      ],
      "metadata": {
        "id": "bU5CUWCuSaXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Google sheets logging setup\n",
        "gsheet_client = None\n",
        "auth.authenticate_user()\n",
        "creds,_ = GoogleAuthDefault()\n",
        "gsheet_client = gspread.authorize(creds)\n",
        "print(\"Google Sheets authentication successful using end-user credentials.\")\n",
        "\n",
        "\n",
        "gsheet_filename = \"BirdCLEF-Experiments\"\n",
        "gsheet_path = \"/content/drive/MyDrive/Kaggle/Bird_CLEF25\"\n",
        "gsheet_fullpath = gsheet_path + gsheet_filename\n",
        "\n",
        "def log_to_gsheet(sheet_name, worksheet_title, data_dict):\n",
        "  if gsheet_client is None:\n",
        "    print(\"Skipping logging since authentication failed!\")\n",
        "    return\n",
        "  try:\n",
        "    sh = gsheet_client.open(sheet_name)\n",
        "\n",
        "    try:\n",
        "      worksheet = sh.worksheet(worksheet_title)\n",
        "    except gspread.exceptions.WorksheetNotFound:\n",
        "      print(f\"Worksheet '{worksheet_title}' not found. Creating it.\")\n",
        "      # Add headers\n",
        "      headers = [str(k) for k in data_dict.keys()]\n",
        "      worksheet = sh.add_worksheet(title=worksheet_title, rows=\"1\",\n",
        "                                   cols=str(len(data_dict) + 5)) # Add extra cols\n",
        "      worksheet.append_row(headers, value_input_option='USER_ENTERED')\n",
        "      print(f\"Created worksheet '{worksheet_title}' with headers: {headers}\")\n",
        "\n",
        "    current_headers = worksheet.row_values(1)\n",
        "    if not current_headers:\n",
        "      current_headers = [str(k) for k in data_dict.keys()]\n",
        "      if not current_headers:\n",
        "        print(\"No headers or data to log!\")\n",
        "        return\n",
        "\n",
        "      worksheet.append_row(current_headers, value_input_option='USER_ENTERED')\n",
        "      print(f\"Appended headers to new worksheet: {current_headers}\")\n",
        "\n",
        "    row_to_append = []\n",
        "    for header in current_headers:\n",
        "      row_to_append.append(data_dict.get(str(header), \"\")) # Get value or empty string if key missing\n",
        "\n",
        "    worksheet.append_row(row_to_append, value_input_option='USER_ENTERED')\n",
        "    print(f\"Logging results to Google Sheet: '{sheet_name}', Worksheet: '{worksheet_title}'\")\n",
        "\n",
        "  except gspread.exceptions.SpreadsheetNotFound:\n",
        "    print(f\"Spreadsheet '{sheet_name}' not found. Recheck name and path!\")\n",
        "  except Exception as e:\n",
        "    print(f\"Error logging to Google Sheet: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "id": "D8T-Onj_BJdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_training_pipeline(model, dataloaders, loss_func, optimizer, scaler, scheduler,\n",
        "                          num_epochs, device, early_stopping_patience=5,\n",
        "                          experiment_name=None, gsheet_worksheet_title=None):\n",
        "  start_time = time.time()\n",
        "  best_val_metric = 0.0\n",
        "  best_model_state = None\n",
        "  epochs_no_improve = 0 # for early stopping\n",
        "\n",
        "  history = {'train_loss': [], 'val_loss': [], 'val_macro_auc': [], 'val_macro_ap': [], 'val_sample_avg_accuracy': []}\n",
        "\n",
        "  # Log experiment parameters once\n",
        "  initial_log_data = {\n",
        "        \"Timestamp\": pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "        \"Experiment_Name\": experiment_name,\n",
        "        \"Epochs_Planned\": num_epochs,\n",
        "        \"Batch_Size\": dataloaders.train.batch_size,\n",
        "        \"Num_Workers\": dataloaders.train.num_workers,\n",
        "        \"Learning_Rate_Initial\": optimizer.param_groups[0]['lr'],\n",
        "        \"Optimizer\": type(optimizer).__name__,\n",
        "        \"Model\": model.__class__.__name__ if hasattr(model, '__class__') else \"N/A\", # Or model.name\n",
        "        \"Scheduler\": type(scheduler).__name__ if scheduler else \"None\",\n",
        "        \"Early_Stopping_Patience\": early_stopping_patience if early_stopping_patience > 0 else \"None\",\n",
        "        # Add more cfg parameters as needed:\n",
        "        \"N_MELS\": cfg.N_MELS,\n",
        "        \"Augmentations_Used\": \"SpecAugment\", # Update this when augmentations are active\n",
        "    }\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    epoch_start_time = time.time()\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    print(f\"\\n--- Epoch {epoch+1}/{num_epochs} (LR: {current_lr:.2e}) ---\")\n",
        "\n",
        "    train_loss = train_one_epoch_amp(model, dataloaders.train, loss_func, optimizer, scaler, device)\n",
        "    history['train_loss'].append(train_loss)\n",
        "    print(f\"Epoch {epoch+1} Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "    val_loss, metrics = validate_one_epoch_amp(model, dataloaders.valid, loss_func, device)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    for key, val in metrics.items(): history[f'val_{key}'].append(val) # Dynamic metric logging\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Val Loss : {val_loss:.4f}\")\n",
        "    for k, v in metrics.items(): print(f\"  Val {k}: {v:.4f}\")\n",
        "\n",
        "    # Scheduler step  (if reduceLROnPlateau, step with val_loss or val_macro_auc)\n",
        "    if scheduler:\n",
        "      if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
        "        scheduler.step(metrics.get('macro_auc', val_loss)) # Step with val metric\n",
        "      else:\n",
        "        scheduler.step() # for schedulers like cosineannealinglr for stepping each epoch\n",
        "\n",
        "    # Checkpoint\n",
        "    current_val_metric = metrics.get('macro_auc', 0.0)\n",
        "    if current_val_metric > best_val_metric:\n",
        "        best_val_metric = current_val_metric\n",
        "        best_model_state = copy.deepcopy(model.state_dict())\n",
        "        print(f\"*** New best Macro AUC: {best_val_metric:.4f}. Saving model state. ***\")\n",
        "        # torch.save(best_model_state, f'/content/best_model_epoch_{epoch+1}.pth') # Optional: save to file\n",
        "        epochs_no_improve = 0 # reset patience\n",
        "    else:\n",
        "      epochs_no_improve += 1\n",
        "\n",
        "    epoch_duration = time.time() - epoch_start_time\n",
        "    print(f\"Epoch {epoch+1} Time: {epoch_duration:.2f}s\")\n",
        "\n",
        "    # Log epoch results\n",
        "    epoch_log_data = {**initial_log_data}\n",
        "    epoch_log_data.update({\n",
        "            \"Epoch_Completed\": epoch + 1,\n",
        "            \"Current_LR\": current_lr,\n",
        "            \"Train_Loss\": train_loss,\n",
        "            \"Val_Loss\": val_loss,\n",
        "            **{f\"Val_{k.replace(' ', '_')}\": v for k, v in metrics.items()}, # Flatten metrics\n",
        "            \"Epoch_Duration_s\": epoch_duration,\n",
        "            \"Best_Val_Macro_AUC_So_Far\": best_val_metric\n",
        "        })\n",
        "    log_to_gsheet(gsheet_filename, gsheet_worksheet_title, epoch_log_data)\n",
        "\n",
        "    # Early stopping\n",
        "    if early_stopping_patience > 0 and epochs_no_improve >= early_stopping_patience:\n",
        "      print(f\"\\nEarly stopping triggered after {early_stopping_patience} epochs with no improvement on Val Macro AUC.\")\n",
        "      break\n",
        "\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "  total_time = time.time() - start_time\n",
        "  print(f\"\\n--- Training Finished ({'Early Stopped' if epochs_no_improve >= early_stopping_patience else 'Completed Epochs'}) ---\")\n",
        "  print(f\"Total Time: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
        "  print(f\"Best Val Macro AUC: {best_val_metric:.4f}\")\n",
        "  if best_model_state: model.load_state_dict(best_model_state)\n",
        "\n",
        "  # Log final summary (optional, or rely on epoch logs)\n",
        "  final_summary_log = {**initial_log_data}\n",
        "  final_summary_log.update({\n",
        "      \"Epoch_Completed\": epoch + 1, # Last completed epoch\n",
        "      \"Best_Val_Macro_AUC_Final\": best_val_metric,\n",
        "      \"Total_Training_Time_s\": total_time,\n",
        "      \"Final_Train_Loss\": history['train_loss'][-1] if history['train_loss'] else None,\n",
        "      \"Final_Val_Loss\": history['val_loss'][-1] if history['val_loss'] else None,\n",
        "  })\n",
        "  log_to_gsheet(gsheet_filename, \"Run_Summaries\", final_summary_log) # Log to a different summary sheet\n",
        "  return model, history"
      ],
      "metadata": {
        "id": "a-3iE6rxQdeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg.EPOCHS = 50\n",
        "EXPERIMENT_NAME = \"ENB0-SpecAugMild-RLROP-ES5-LR1e-3-50\"\n",
        "\n",
        "if 'optimizer' in globals():\n",
        "  #scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.EPOCHS, eta_min=1e-6)\n",
        "  print(f\"Scheduler initialized: {type(scheduler).__name__}\")\n",
        "else:\n",
        "  scheduler = None\n",
        "  print(\"Optimizer not found, proceed without LR scheduler\")\n",
        "\n",
        "print(f\"\\nStarting training: {cfg.EPOCHS} epochs, BS={cfg.BATCH_SIZE}, Workers={cfg.NUM_WORKERS}\")\n",
        "\n",
        "\n",
        "# Train\n",
        "#if 'dataloaders' in globals() and dataloaders is not None and 'model' in globals() and 'criterion' in globals() and 'optimizer' in globals() and 'scaler' in globals():\n",
        "trained_model, training_history = run_training_pipeline(\n",
        "      model,\n",
        "      dataloaders,\n",
        "      loss_func,\n",
        "      optimizer,\n",
        "      scaler,\n",
        "      scheduler,\n",
        "      num_epochs=cfg.EPOCHS,\n",
        "      device=device,\n",
        "      early_stopping_patience=5,\n",
        "      experiment_name=EXPERIMENT_NAME,\n",
        "      gsheet_worksheet_title=\"Epoch-logs\"\n",
        "  )\n",
        "#else:\n",
        "#  print(\"Error: Dataloaders not initialized, re-run the pipeline.\")\n",
        "\n",
        "# --- Plot Results ---\n",
        "if 'training_history' in globals() and training_history['train_loss']:\n",
        "    epochs_completed = len(training_history['train_loss'])\n",
        "    epochs_range = range(1, epochs_completed + 1)\n",
        "    plt.figure(figsize=(17, 5))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(epochs_range, training_history['train_loss'], 'bo-', label='Training Loss')\n",
        "    plt.plot(epochs_range, training_history['val_loss'], 'ro-', label='Validation Loss')\n",
        "    plt.title('Loss'); plt.xlabel('Epochs'); plt.ylabel('Loss'); plt.legend(); plt.grid(True)\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(epochs_range, training_history['val_macro_auc'], 'go-', label='Val Macro AUC')\n",
        "    plt.title('Macro AUC'); plt.xlabel('Epochs'); plt.ylabel('AUC'); plt.legend(); plt.grid(True)\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(epochs_range, training_history['val_sample_avg_accuracy'], 'mo-', label='Val Sample Avg Accuracy')\n",
        "    plt.title('Sample Avg Accuracy'); plt.xlabel('Epochs'); plt.ylabel('Accuracy'); plt.legend(); plt.grid(True)\n",
        "    plt.tight_layout(); plt.show()\n",
        "else:\n",
        "    print(\"No training history to plot (training might have been interrupted or too short).\")"
      ],
      "metadata": {
        "id": "ctKopoz3Qdbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(trained_model.state_dict(), '/content/drive/MyDrive/Kaggle/Bird_CLEF25/models/ENB0-SpecAugMild-RLROP-ES5-LR1e-3-50.pth')\n"
      ],
      "metadata": {
        "id": "BO_Itd7eU8qO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6n0q5DS18ub8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}